{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of context | ABIDE dataset\n",
    "\n",
    "\n",
    "- [The ABIDE dataset](#The-ABIDE-dataset):\n",
    "    - [Load the dataset](#Load-the-dataset)\n",
    "    - [Data Exploration](#Data-Exploration)\n",
    "    - [Explore the dataset: Visualization](#Explore-the-dataset:-Visualization)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ABIDE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays the Autism Spectrum Disorder is an hot topic in research.\n",
    "The fondamental issue to be assessed is the absence of a Gold Standard methodology for diagnosis evaluation: based on clinical interviews and behavioral assessments.\n",
    "\n",
    "In order to improve knowledge and discover rules behind ASD, ABIDE Dataset has been borned.\n",
    "\n",
    "The ABIDE Dataset is a collaborative dataset, multi-sites, containing several kinds of informations: fMRI, sMRi, phenotypic, diagnosis.\n",
    "\n",
    "This project has been focused on Phenotypic characteristics dealing with the fondamental issue mentioned above.\n",
    "\n",
    "The ABIDE Dataset has been validated in several papers.\n",
    "Our reference and starting poin is available here:\n",
    "https://pubmed.ncbi.nlm.nih.gov/30393630/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #to interact to the file system\n",
    "import numpy as np #Statistics\n",
    "import pandas as pd #Database Technology <-> Data preproc & Data Analysis\n",
    "from matplotlib import pyplot as plt #Visualization\n",
    "import seaborn as sns #Visualization\n",
    "import missingno as msno\n",
    "import random \n",
    "from sklearn.preprocessing import RobustScaler #scikit-learn -> ML\n",
    "import OurFunctions as of #saperated collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_phenotypic.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 1112 subjects and has 74 features.\n",
    "\n",
    " At first look, it is noticeable the presence of categorical and numerical features and missing values. Moreover, we can see the presence of the categories DX_GROUP and DSM_IV_TR, that are described from the ABIDE dataset legend as diagnostic, so we will further remove them from the dataset for the model predictor construction. \n",
    " We can also proceed deleting the feature EYE_STATUS_AT_SCAN, because it deals with the eyes opening by the subject  during the fMRI, thus it is meaningless for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DX_GROUP and DSM_IV_TR are our targets.\n",
    "DX_GROUP contains info about ASD detection (yes/no). \n",
    "DSM_IV_TR specifyes which kind of autism, if there is.\n",
    "However, in our investigation, we are interested only in evaluating the presence or not of the disorder, so we will not consider the information in DSM_IV_TR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the balancing of our dataset, we check the target feature: 'DX_GROUP'. \n",
    "\n",
    "1 | Controls\n",
    "\n",
    "2 | Autism detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.evaluate_balancing(ASD_phenotypic_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DX_GROUP is balanced enought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to have a better view of the information contained in the dataset we display the names of the features and the respective types and quantity of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a huge amount of missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a better view of the distribution of the null values, we check the presence of missing values catalogated as None or numpy.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the missing values\n",
    "\n",
    "nan_values = ASD_phenotypic_original.isna().sum() \n",
    "nan_sorted = nan_values.sort_values(ascending=False)\n",
    "\n",
    "total_rows = ASD_phenotypic_original.shape[0]\n",
    "percent_missing = (nan_sorted / total_rows) * 100\n",
    "\n",
    "# We implemented a function \"select_columns\", that is able to define wich columns are numerical\n",
    "# and which ones are categorical (also redefine the objects as categorical in the dataset)\n",
    "numeric_columns, categorical_columns, ASD_phenotypic_original = of.select_columns(ASD_phenotypic_original)\n",
    "\n",
    "# We plot the distribution of missing values, with the specification of numeric and categorical columns\n",
    "of.plot_missing_values(percent_missing, numeric_columns, legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the majority of the features the amount of missing values is not depreciable, so we can say that the information that is stored in the feature is not enough to create a reliable classier/cluster algortihm based on it. The same comment can be done for the subjects. We need to work on a dataset that has a maximum of 10% of missing values per feature, so we need to clean this in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look on the general statistics for the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the presence of \"-9999\" as minimum value for different features, a value that is commonly used to denote missing data or values out of range, so it should be better to consider them as NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an overall view of the dataset, let's start to work on it in order to clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we decided to apart the features that give the diagnosis of the subjects (DX_GROUP and DSM_IV_TR), in order to use them as the control labels. \n",
    "\n",
    "We also drop EYE_STATUS_AT_SCAN, that it's not relevant feature for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ASD_phenotypic = ASD_phenotypic_original.drop(columns=['DSM_IV_TR','EYE_STATUS_AT_SCAN'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we decide to drop SUB_ID, as it only store the information about the ID of the subject. But before we check if there aren't replicated subjects. Then if we don't find any duplicate, we can simply drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicate values in the 'SUB_ID' column\n",
    "duplicate_ids = ASD_phenotypic['SUB_ID'].duplicated(keep=False)\n",
    "\n",
    "# Get the unique duplicate IDs\n",
    "unique_duplicate_ids = ASD_phenotypic.loc[duplicate_ids, 'SUB_ID'].unique()\n",
    "\n",
    "#Drop column if there aren't duplicates\n",
    "if len(unique_duplicate_ids) == 0:\n",
    "    ASD_phenotypic = ASD_phenotypic.drop(columns=['SUB_ID'])\n",
    "    print(\"SUB_ID was dropped\")\n",
    "else:\n",
    "    print(\"There are replicated values:\" + str(unique_duplicate_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a real count of the amount of missing values per feature, we change the -9999 values present in the overall data to np.NaN (we can make this because we know from the datasheets that -9999 is a value out of range for all the features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ASD_phenotypic:\n",
    "    \n",
    "    # Replace -9999 and \"-9999\" with NaN\n",
    "    ASD_phenotypic[column] = ASD_phenotypic[column].replace(['-9999', -9999], np.NaN)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that there are two features that indicates if the personal that took the tests is a trained professional or not (ADI_R_RSRCH_RELIABLE and ADOS_RSRCH_RELIABLE ). This is information is interesting to determine if the scores achieved in the respective tests are reliable or not. In order to avoid include not reliable information, we decide to eliminate the subjects that has a 0 in both categories (not reliable), while if the value is 1 or missed, we maintain the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for rows to keep\n",
    "filter = (ASD_phenotypic['ADI_R_RSRCH_RELIABLE'] != 0) | (ASD_phenotypic['ADOS_RSRCH_RELIABLE'] != 0)\n",
    "\n",
    "# Calculate the number of subjects to delete\n",
    "deleted_subjects = len(ASD_phenotypic) - filter.sum()\n",
    "\n",
    "# Apply the mask to both DataFrames\n",
    "ASD_phenotypic= ASD_phenotypic[filter]\n",
    "\n",
    "# Print the number of subjects deleted\n",
    "print(\"Number of subjects deleted:\", deleted_subjects)\n",
    "\n",
    "ASD_phenotypic = ASD_phenotypic.drop(columns=['ADI_R_RSRCH_RELIABLE','ADOS_RSRCH_RELIABLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clean our starting dataset:\n",
    "\n",
    "- displaying the distribution of missing values\n",
    "- menaging our dataset with the purpose to achieve at a cleaned one with teo conditions:\n",
    "    \n",
    "    - 10 % of NaN per feature\n",
    "    - at least 1/4 of subjects involved\n",
    "    - balancing of dataset (max 60/40 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Visualizzazione dei missing values\n",
    "msno.matrix(ASD_phenotypic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our aim, following the pipeline of reference paper, we create an alghoritm with boundary contitions already mentioned. Moreover, we force the presence of key features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features chiave da mantenere\n",
    "key_features = ['FIQ', 'VIQ', 'PIQ', 'ADI_R_VERBAL_TOTAL_BV', 'ADOS_TOTAL']\n",
    "\n",
    "def remove_high_missing(df, key_features, balance_column, min_subjects=200, max_missing_percentage=10):\n",
    "    current_df = df.copy()\n",
    "    \n",
    "    # Calcolare la proporzione iniziale del bilanciamento\n",
    "    initial_balance = current_df[balance_column].value_counts(normalize=True)\n",
    "    \n",
    "    while (of.calculate_missing_percentage(current_df) > max_missing_percentage): \n",
    "        # Calcola la percentuale di valori mancanti per ciascuna colonna e riga\n",
    "        missing_percent_features = current_df.isna().mean() * 100\n",
    "        missing_percent_subjects = current_df.isna().mean(axis=1) * 100\n",
    "        \n",
    "        # Filtra le features chiave per non rimuoverle\n",
    "        non_key_features = missing_percent_features.drop(labels=key_features, errors='ignore')\n",
    "        \n",
    "        # Trova la feature o il soggetto con il più alto tasso di missing values\n",
    "        max_feature_missing = non_key_features.max()\n",
    "        max_subject_missing = missing_percent_subjects.max()\n",
    "        \n",
    "        # Rimuovi la feature o il soggetto con il tasso di missing values più alto\n",
    "        if max_feature_missing >= max_subject_missing and not non_key_features.empty:\n",
    "            feature_to_drop = non_key_features.idxmax()\n",
    "            current_df = current_df.drop(columns=[feature_to_drop])\n",
    "            print(f\"Rimosso feature: {feature_to_drop}\")\n",
    "        elif not missing_percent_subjects.empty:\n",
    "            scelto_soggetto = False\n",
    "            while not scelto_soggetto:\n",
    "                subject_to_drop = missing_percent_subjects.idxmax()\n",
    "                temp_df = current_df.drop(index=[subject_to_drop])\n",
    "            \n",
    "             # Verifica il bilanciamento dopo la rimozione\n",
    "                current_balance = temp_df[balance_column].value_counts(normalize=True)\n",
    "                if all(abs(initial_balance - current_balance) <= 0.2):  # Assicurarsi che il bilanciamento non cambi di più del 20%\n",
    "                    current_df = temp_df\n",
    "                    scelto_soggetto = True\n",
    "                    print(f\"Rimosso soggetto: {subject_to_drop}\")\n",
    "                else:\n",
    "                    id = missing_percent_subjects.drop(subject_to_drop)\n",
    "                    print(f\"Soggetto non rimosso per mantenere il bilanciamento: {subject_to_drop}\")\n",
    "            \n",
    "\n",
    "        # Controllo dello stato attuale del DataFrame\n",
    "        print(f\"Percentuale attuale di missing values: {of.calculate_missing_percentage(current_df):.2f}%\")\n",
    "        if current_df.shape[0] > min_subjects:\n",
    "            print(f\"Numero di soggetti rimanenti: {current_df.shape[0]}\")\n",
    "    \n",
    "    return current_df\n",
    "\n",
    "\n",
    "# Applica la funzione di pulizia sul DataFrame\n",
    "ASD_phenotypic_cleaned = remove_high_missing(ASD_phenotypic, key_features, balance_column='DX_GROUP', min_subjects=200, max_missing_percentage=10)\n",
    "\n",
    "# Mostra DataFrame pulito\n",
    "ASD_phenotypic_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Missin Values for features and subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_cleaned.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the missing values\n",
    "\n",
    "nan_values = ASD_phenotypic_cleaned.isna().sum() \n",
    "nan_sorted = nan_values.sort_values(ascending=False)\n",
    "\n",
    "total_rows = ASD_phenotypic_cleaned.shape[0]\n",
    "percent_missing = (nan_sorted / total_rows) * 100\n",
    "\n",
    "# We implemented a function \"select_columns\", that is able to define wich columns are numerical\n",
    "# and which ones are categorical (also redefine the objects as categorical in the dataset)\n",
    "numeric_columns, categorical_columns, ASD_phenotypic_cleaned = of.select_columns(ASD_phenotypic_cleaned)\n",
    "\n",
    "# We plot the distribution of missing values, with the specification of numeric and categorical columns\n",
    "of.plot_missing_values(percent_missing, numeric_columns, legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo dei valori mancanti per soggetto anziché per feature\n",
    "nan_values_per_subject = ASD_phenotypic_cleaned.T.isna().sum()\n",
    "\n",
    "# Ordinamento dei valori mancanti\n",
    "subjects_with_nan_sorted = nan_values_per_subject.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "of.plot_missing_values(subjects_with_nan_sorted, nan_values_per_subject, legend=False)\n",
    "plt.ylabel('Subjects')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.evaluate_balancing(ASD_phenotypic_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved our goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store them in a new dataset called ASD_clinical\n",
    "ASD_clinical = ASD_phenotypic_cleaned[['DX_GROUP']]\n",
    "# Drop  columns DX_GROUP e fai storage a parte\n",
    "ASD_phenotypic = ASD_phenotypic_cleaned.drop(columns=['DX_GROUP'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check how our features are distributed, in order to know which kind of normalization of the data is more suitable and if we may need to delete some outliers during the execution of our classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per plottare le distribuzioni delle features\n",
    "of.plot_distributions(ASD_phenotypic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disucussing graph's results:\n",
    "We notice the presence of possible outliers per feature. Thus, we quantify the fraction in the entire dataset, in order to take in consideration this element in our future analysis (Outlier detection in classification). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'AGE_AT_SCAN': {'threshold': 45, 'rule': 'greater'},\n",
    "    'FIQ': {'threshold': 50, 'rule': 'less'},\n",
    "    'VIQ': {'threshold': 170, 'rule': 'greater'},\n",
    "    'PIQ': {'threshold': 50, 'rule': 'less'},\n",
    "    'ADI_R_VERBAL_TOTAL_BV': {'threshold': 6, 'rule': 'less'}\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of outliers per feature\n",
    "outlier_counts = {}\n",
    "total_samples = len(ASD_phenotypic)\n",
    "\n",
    "for feature, config in thresholds.items():\n",
    "    threshold = config['threshold']\n",
    "    rule = config['rule']\n",
    "    if rule == 'greater':\n",
    "        outlier_count = (ASD_phenotypic[feature] > threshold).sum()\n",
    "    elif rule == 'less':\n",
    "        outlier_count = (ASD_phenotypic[feature] < threshold).sum()\n",
    "    outlier_counts[feature] = outlier_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % outliers per feature\n",
    "outlier_percentages = {feature: (count / total_samples) * 100 for feature, count in outlier_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for feature, percentage in outlier_percentages.items():\n",
    "    print(f'Feature: {feature}, Outlier Percentage: {percentage:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform correlation analysis separatamente, per numeriche e categoriche così da eliminarne alcune. Poi o ANOVA TEST O BOX PLOT O CAPIAMO PER TROVARE EVENTUALE CORRELAZIONE TRA NUMERICHE E CATEGORICHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we need to normalize the data to make comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selezione delle colonne numeriche e categoriche\n",
    "numeric_columns = ASD_phenotypic.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_columns = ASD_phenotypic.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "# Inizializzazione dello StandardScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Addestramento dello scaler e trasformazione dei dati numerici\n",
    "scaled_numeric_data = scaler.fit_transform(numeric_columns)\n",
    "\n",
    "# Creazione di un nuovo DataFrame con i dati numerici normalizzati\n",
    "numeric_columns_normalized = pd.DataFrame(scaled_numeric_data, columns=numeric_columns.columns, index=ASD_phenotypic.index)\n",
    "\n",
    "# Combina i dati numerici normalizzati e i dati categorici in un unico DataFrame\n",
    "ASD_phenotypic_normalized = pd.concat([numeric_columns_normalized, categorical_columns], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_normalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_normalized = ASD_phenotypic_normalized.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calcola la correlation matrix\n",
    "correlation_matrix = numeric_normalized.corr()\n",
    "\n",
    "# Visualizza la correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION MATRIX\n",
    "# Calcola la correlazione tra le features normalizzate\n",
    "correlation_matrix = numeric_normalized.corr()\n",
    "numeric_normalized.T\n",
    "f,ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(numeric_normalized.corr(), \n",
    "            annot=True, \n",
    "            linewidths=.5, \n",
    "            fmt= '.2f',\n",
    "            ax=ax,\n",
    "            vmin=-1, \n",
    "            vmax=1,\n",
    "            cmap = \"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns, categorical_columns, ASD_phenotypic_normalized = of.select_columns(ASD_phenotypic_normalized)\n",
    "# Compute Cramer's V for every pair of features\n",
    "cramer_v_scores = pd.DataFrame(index=categorical_columns, columns=categorical_columns)\n",
    "for feature1 in categorical_columns:\n",
    "    for feature2 in categorical_columns:\n",
    "        cramer_v = of.cramers_v(ASD_phenotypic_normalized[feature1], ASD_phenotypic_normalized[feature2])\n",
    "        cramer_v_scores.loc[feature1, feature2] = cramer_v\n",
    "\n",
    "# Plot heatmap of Cramer's V scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramer_v_scores.astype(float), \n",
    "            annot=True, \n",
    "            linewidths=.5, \n",
    "            fmt='.2f',\n",
    "            cmap=\"coolwarm\")\n",
    "plt.title(\"Cramer's V scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the information contained in VIQ_TEST_TYPE and PIQ_TEST_TYPE is almost the same, so it's reasonable to delete one of them. We decide to drop VIQ_TEST_TYPE and minimize the correlation between test types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual \"correlation\" between Categorical and Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to evaluate eventual correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_columns, categorical_columns, ASD_phenotypic= of.select_columns(ASD_phenotypic)\n",
    "\n",
    "# Number of plots per row\n",
    "plots_per_row = 4\n",
    "num_plots = len(numeric_columns) * len(categorical_columns)\n",
    "num_rows = (num_plots + plots_per_row - 1) // plots_per_row  # Calculate the number of rows needed\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(20, num_rows * 5))\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for numeric_col in numeric_columns:\n",
    "    for cat_col in categorical_columns:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        sns.boxplot(x=cat_col, y=numeric_col, data=ASD_phenotypic,\n",
    "                    order=ASD_phenotypic_normalized.groupby(cat_col, observed=False)[numeric_col].median().sort_values().index, ax=ax)\n",
    "        \n",
    "        # Highlight the median in red\n",
    "        median = ASD_phenotypic.groupby(cat_col, observed=False)[numeric_col].median().sort_values()\n",
    "        for i in range(len(median)):\n",
    "            ax.plot(i, median.iloc[i], 'ro')\n",
    "        \n",
    "        ax.set_title(f'Boxplot of {numeric_col} by {cat_col}')\n",
    "        ax.set_xlabel(cat_col)\n",
    "        ax.set_ylabel(numeric_col)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(plot_idx, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform correlation analysis separatamente, per numeriche e categoriche così da eliminarne alcune. Poi o ANOVA TEST O BOX PLOT O CAPIAMO PER TROVARE EVENTUALE CORRELAZIONE TRA NUMERICHE E CATEGORICHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, in order to avoid mismatch between attributes that are the same, but wrote with upper or lower characters, we decide to unify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make all the caracters upper for all the categorical features\n",
    "category_columns_upper = ASD_phenotypic.select_dtypes(include='category').apply(lambda x: x.str.upper())\n",
    "\n",
    "#We now modify them in the dataset\n",
    "ASD_phenotypic[category_columns_upper.columns] = category_columns_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns, categorical_columns, ASD_phenotypic = of.select_columns(ASD_phenotypic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain the names of the features \n",
    "categorical_column_names = categorical_columns.tolist()\n",
    "categorical_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SITE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SITE_ID refers to the place where the data from the subject was recluted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesso a una specifica colonna categorica utilizzando la lista di nomi\n",
    "specific_category_column = ASD_phenotypic[categorical_column_names[0]].value_counts(dropna=False)\n",
    "specific_category_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is data that has been collected from the same center that we decide to unify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to replace the categories for the indicated cases\n",
    "\n",
    "def replace_categories(category):\n",
    "    if \"UCLA\" in category:\n",
    "        return \"UCLA\"\n",
    "    if \"LEUVEN\" in category:\n",
    "        return \"LEUVEN\"\n",
    "    if \"UM\" in category:\n",
    "        return \"UM\"\n",
    "    else:\n",
    "        return category\n",
    "\n",
    "# Then we apply the replace function\n",
    "ASD_phenotypic[categorical_column_names[0]] = ASD_phenotypic[categorical_column_names[0]].apply(replace_categories).astype('category')\n",
    "\n",
    "# Now we check the new order\n",
    "specific_category_column = ASD_phenotypic[categorical_column_names[0]].value_counts(dropna=False)\n",
    "specific_category_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIQ_TEST_TYPE, VIQ_TEST_TYPE and PIQ_TEST_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIQ_TEST_TYPE, VIQ_TEST_TYPE and PIQ_TEST_TYPE refers to the type of test that each center chose to get the information of FIQ_TEST, VIQ_TEST and PIQ_TEST respectively. As we want our clustering algorithm to be as most general as possible, we want to be able to categorize subjects in despise of the test used by the centers to get the data. So we decide to drop this feature as well.\n",
    "\n",
    "Note that if in a future we will be interested in to analyze if there are differences between the clustering score obtained using the result for each difference test we'll can retrieve the information opportunely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,4):\n",
    "    specific_category_column = ASD_phenotypic[categorical_column_names[i]].value_counts(dropna=False)\n",
    "    print(specific_category_column)\n",
    "    print('______________________________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to replace the categories for the indicated cases\n",
    "\n",
    "def replace_categories(category):\n",
    "    if pd.isna(category):  # Controlla se il valore è NaN\n",
    "        return category  # Se è NaN, restituisci lo stesso valore\n",
    "    if \"WASI\" in category:\n",
    "        return \"WASI\"\n",
    "    if \"WISC\" in category:\n",
    "        return \"WISC\"\n",
    "    if \"WAIS\" in category:\n",
    "        return \"WAIS\"\n",
    "    if \"DAS\" in category:\n",
    "        return \"DAS\"\n",
    "    if \"HAWIK\" in category:\n",
    "        return \"HAWIK\"\n",
    "    if \"PPVT\" in category:\n",
    "        return \"PPVT\"\n",
    "    if \"RAVENS\" in category:\n",
    "        return \"RAVENS\"\n",
    "   \n",
    "    else:\n",
    "        return category\n",
    "\n",
    "for i in range (1,4):\n",
    "    ASD_phenotypic[categorical_column_names[i]] = ASD_phenotypic[categorical_column_names[i]].apply(replace_categories).astype('category')\n",
    "    specific_category_column = ASD_phenotypic[categorical_column_names[i]].value_counts(dropna=False)\n",
    "    print(specific_category_column)\n",
    "    print('______________________________________\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to fullfill the missing values for all the features, based on an analysis of the information delivered by each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQ Test Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use features FIQ, VIQ and PIQ in order to fill some values in FIQ-TEST-TYPE, VIQ-TEST-TYPE, PIQ-TEST-TYPE.\n",
    "Since the presence of more missing values in \"Type\" features, we make a comparison for each couple of features. For instance: if for FIQ there is a value and for FIQ-TEST-TYPE there is a missing one, we fill it with the MODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista delle coppie di features da controllare\n",
    "feature_pairs = [\n",
    "    ('FIQ_TEST_TYPE', 'FIQ'),\n",
    "    ('PIQ_TEST_TYPE', 'PIQ'),\n",
    "    ('VIQ_TEST_TYPE', 'VIQ')]\n",
    "\n",
    "# Iteriamo su ogni coppia di features\n",
    "for test_type_col, score_col in feature_pairs:\n",
    "    # Iteriamo su ogni riga del DataFrame\n",
    "    for index, row in ASD_phenotypic.iterrows():\n",
    "        # Controlliamo se il valore nella colonna 'test_type_col' è mancante\n",
    "        if pd.isnull(row[test_type_col]):\n",
    "            # Se il valore nella colonna 'score_col' è presente\n",
    "            if not pd.isnull(row[score_col]):\n",
    "                # Calcoliamo la moda di 'test_type_col'\n",
    "                mode_test_type = ASD_phenotypic[test_type_col].mode()[0]\n",
    "                # Sostituiamo il valore mancante nella colonna 'test_type_col' con la moda\n",
    "                ASD_phenotypic.at[index, test_type_col] = mode_test_type\n",
    "            # Se entrambi i valori in 'test_type_col' e 'score_col' sono mancanti\n",
    "            elif pd.isnull(row[score_col]):\n",
    "                # Verifichiamo se \"NOT_AVAILABLE\" è già presente tra le categorie della colonna\n",
    "                if \"NOT_AVAILABLE\" not in ASD_phenotypic[test_type_col].cat.categories:\n",
    "                    # Aggiungiamo \"NOT_AVAILABLE\" come nuova categoria\n",
    "                    ASD_phenotypic[test_type_col] = ASD_phenotypic[test_type_col].cat.add_categories(\"NOT_AVAILABLE\")\n",
    "                # Assegniamo la categoria 'NOT_AVAILABLE' a 'test_type_col'\n",
    "                ASD_phenotypic.at[index, test_type_col] = 'NOT_AVAILABLE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to fill the missing values, we note that as the data for the variables FIQ, VIQ, PIQ was obtained with different tests, there are also different scales for the scores to take into account. In this way, we prefer to apply a standardization so we have all the score on the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For FIQ, the score scale is between 30-170 if the test taken was \"DAS\", otherwise is 50-160.\n",
    "#We will unify all the data to the larger scale, i.e. 50-160\n",
    "\n",
    "# We start defining the condition\n",
    "condition = (ASD_phenotypic['FIQ_TEST_TYPE'] == 'DAS') | (ASD_phenotypic['FIQ'] < 50) | (ASD_phenotypic['FIQ'] > 160)\n",
    "\n",
    "# Then we standarize the values dictated by the condition, to the new scale\n",
    "ASD_phenotypic['FIQ'] = np.where(condition, \n",
    "                        (ASD_phenotypic['FIQ'] - 30) / (170 - 30) * (160 - 50) + 50, \n",
    "                        ASD_phenotypic['FIQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For VIQ, the score scale is between 31-169 if the test taken was \"DAS\", \n",
    "#between 36-164 if the test taken was \"STANFORD\",\n",
    " #between 40-160 if the test taken was \"PPVT\",  otherwise is 50-160.\n",
    "#We will unify all the data to the more common used scale, i.e. 50-160\n",
    "\n",
    "for i in ASD_phenotypic.index:\n",
    "    test_type = ASD_phenotypic['VIQ_TEST_TYPE'][i]\n",
    "    current_value = ASD_phenotypic['VIQ'][i]\n",
    "    if (test_type == 'DAS') or (current_value <36) or (current_value > 164):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 31) / (169 - 31) * (160 - 50) + 50\n",
    "    elif (test_type == 'STANFORD') or (current_value <40) or (current_value > 160):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 36) / (164 - 36) * (160 - 50) + 50\n",
    "    elif (test_type == 'PPVT') or (current_value <50) or (current_value > 160):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 40) / (160 - 40) * (160 - 50) + 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PIQ, the score scale is between 31-166 if the test taken was \"DAS\", \n",
    "#between 36-164 if the test taken was \"STANFORD\",\n",
    " #between 50-160 if the test taken was \"RAVENS\",  otherwise is 53-160.\n",
    "#We will unify all the data to the more common used scale,, i.e. 50-160\n",
    "\n",
    "for i in ASD_phenotypic.index:\n",
    "    test_type = ASD_phenotypic['PIQ_TEST_TYPE'][i]\n",
    "    current_value = ASD_phenotypic['PIQ'][i]\n",
    "    if (test_type == 'DAS') or (current_value <36) or (current_value > 164):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 31) / (166 - 31) * (160 - 50) + 50\n",
    "    elif (test_type == 'STANFORD') or (current_value <50) or (current_value > 160):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 36) / (164 - 36) * (160 - 50) + 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADOS_TOTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature \"ADOS_TOTAL\" is simply the sum of the scores obtained by \"ADOS_COMM\" and \"ADOS_SOCIAL\", so we can reduce the amount of missing values using the values of those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9999 and \"-9999\" with NaN\n",
    "ASD_phenotypic_original[\"ADOS_COMM\"] = ASD_phenotypic_original[\"ADOS_COMM\"].replace(['-9999', -9999], np.NaN)\n",
    "ASD_phenotypic_original[\"ADOS_SOCIAL\"] = ASD_phenotypic_original[\"ADOS_SOCIAL\"].replace(['-9999', -9999], np.NaN)\n",
    "\n",
    "for i in ASD_phenotypic[\"ADOS_TOTAL\"].index:\n",
    "    ados_comm = ASD_phenotypic_original[\"ADOS_COMM\"][i]\n",
    "    ados_social = ASD_phenotypic_original[\"ADOS_SOCIAL\"][i]\n",
    "    if not pd.isna(ados_comm) and not pd.isna(ados_social):\n",
    "        ASD_phenotypic.loc[i, \"ADOS_TOTAL\"] = ados_comm + ados_social\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test scores filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that to fill the missing values of the test subministred\n",
    "it should be good to rely on the standard score achieved by the mean\n",
    "of the global population (if the statistics are available in the literature) or the cutoff for the diagnostic of ASD,\n",
    "otherwise we will use the mean extracted from our dataset.\n",
    "\n",
    "So for the features \"FIQ\", \"VIQ\", \"PIQ\", \"ADOS_TOTAL\", \"ADI_R_VERBAL_TOTAL_BV\", we will apply a custom function that checks if there is an available value in literature for the mundial mean, otherwise assign the mean of the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of features that we want to fill\n",
    "test_score_fatures = [\"FIQ\", \"VIQ\", \"PIQ\", \"ADOS_TOTAL\", \"ADI_R_VERBAL_TOTAL_BV\"]\n",
    "\n",
    "#function to fill with the global mean or the data feature mean\n",
    "def test_score_fill (feature_value, feature_name, feature_mean):\n",
    "    # We create a dictionary to store the literature mean scores\n",
    "    literature_scores = {\n",
    "    \"FIQ\": list(range(95, 100)), # EEUU, mean score retrieved from https://www.worlddata.info/iq-by-country.php\n",
    "    \"VIQ\": list(range(95, 100)), # EEUU, mean score retrieved from https://www.worlddata.info/iq-by-country.php\n",
    "    \"PIQ\": list(range(95, 100)), # EEUU, mean score retrieved from https://www.worlddata.info/iq-by-country.php\n",
    "    \"ADOS_TOTAL\": list(range(6, 12)), # autism cutoff retrieved from https://www.researchgate.net/figure/ADOS-maximum-score-and-cut-off-points-for-ASD-15_tbl1_361212648\n",
    "    \"ADI_R_VERBAL_TOTAL_BV\": list(range(7, 10)), # autism cutoff retrieved from https://www.researchgate.net/figure/Summary-statistics-for-ADI-R-domain-scores_tbl4_6709395\n",
    "    }\n",
    "\n",
    "    # Then we check which feature we obtained to decide if replace\n",
    "    # using the value in the dictionary ot directly the mean of the data\n",
    "    if pd.isna(feature_value):\n",
    "\n",
    "        if feature_name in literature_scores:\n",
    "            return random.choice(literature_scores[feature_name])\n",
    "        else:\n",
    "            return feature_mean\n",
    "    else:\n",
    "\n",
    "        return feature_value\n",
    "\n",
    "#loop for filling the features   \n",
    "for feature_name in test_score_fatures:\n",
    "    feature_mean = ASD_phenotypic_original[feature_name].mean()\n",
    "    ASD_phenotypic[feature_name] = ASD_phenotypic[feature_name].apply(test_score_fill, args=(feature_name, feature_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that if we display the information of the dataset, we have no longer presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ASD_phenotypic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check how our features are distributed, in order to know which kind of normalization of the data is more suitable and if we may need to delete outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per plottare le distribuzioni delle features\n",
    "of.plot_distributions(ASD_phenotypic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESSED DATA STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We decide to store in a file .csv the pre-processed dataset\n",
    "ASD_phenotypic.to_csv('DataSets/Phenotypic Datasets/ASD_phenotypic_preprocessed.csv', index=False)\n",
    "# And also the diagnostic groups\n",
    "ASD_clinical.to_csv('DataSets/Phenotypic Datasets/ASD_clinical.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
