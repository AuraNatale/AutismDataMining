{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# from our documents\n",
    "import OurFunctions as of\n",
    "\n",
    "# from Scikit Learn library\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, make_scorer, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import plot_tree \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# from Imb Learn\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start importing our datasets, where ASD_phenotypic_original is the untouched dataset, ASD_phenotypic is the one proceding from the preprocessing and where the ASD_diagnosis dataset contains its respectives DX_GROUP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_phenotypic.csv'))\n",
    "ASD_phenotypic = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_phenotypic_preprocessed.csv'))\n",
    "ASD_diagnosis = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_clinical.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we split the data into train set and test set, to be sure that our classifier is evaluated on data that it had never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(ASD_phenotypic, ASD_diagnosis['DX_GROUP'], test_size=0.3, random_state=42)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check the balance between classes for each set, to  be sure we are training the classifier using a good split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_test = y_test.value_counts(normalize=True)\n",
    "class_counts_train = y_train.value_counts(normalize=True)\n",
    "class_count_train_num = y_train.value_counts()\n",
    "\n",
    "# Print class count DX_GROUP\n",
    "print(\"Class proportions for:\")\n",
    "print(\"- test set: \" + str(class_counts_test))\n",
    "print(\"- train set: \" + str(class_counts_train))\n",
    "print(\"Num subjects involved per \"+ str(class_count_train_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix the umbalancing for the training phase, we use SMOTENC (variable of SMOTE that can handle categorical features), to create fictious subjects and rebalance them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "categorical_features = categorical_columns.tolist()\n",
    "\n",
    "# Initializing SMOTE-NC \n",
    "sampler = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "\n",
    "# Applying SMOTE-NC in order to generate new syntetic samples\n",
    "X_SMOTE, Y_SMOTE = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print class distribution after SMOTE-NC application\n",
    "print(pd.Series(Y_SMOTE).value_counts(normalize=True))\n",
    "print(\"Num subjects involved per\")\n",
    "print(pd.Series(Y_SMOTE).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is unbalanced, having 60% of the subjects with autism condition. However, the split separates train and test sets respecting the proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline receive as mandatory arguments the dataset, the target and the chosen classifier, it also has some optional parameters that can specify some actions or parameters.\n",
    "\n",
    "- To achieve a more reliable accuracy score we use k-fold cross validation. In this way we are performing the training on different train and validation fold and we avoid to have a better accuracy based only on the subjects we selected (k of cv can be set up from the outside).\n",
    "- There exist the option to select if to use an encoder or a scaler (by default both options are set to true).\n",
    "- The pipeline execute grid search to find the best hyperparameters per our training.\n",
    "- The grid search can use both default parameters present in the function or gived by the useer as well.\n",
    "- The algorithm can handle three different types of classifier: KNearestNeighbors, RandomForestClassifier and LinearSupportVectorMachine.\n",
    "- The feature selection can be integrated to the pipeline if requested.\n",
    "\n",
    "The output is composed by the trained model, the best_parameters found by the grid search, the best achieved accuracy, and if in case the feature_selector is activated, also the selected_features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_pipeline(dataset, target, classifier, encoder = True, scaler = True, parameters_grid_search = None, cv = None, feature_selector=False,  parameters_feature_selector = None):\n",
    "\n",
    "    # Defining metrics to be used as scoring \n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'f1_score': make_scorer(f1_score)\n",
    "    }\n",
    "\n",
    "    # Preprocess to make on the train data may include\n",
    "    # - normalization of the numerical columns\n",
    "    # - one hot encoding on the categorical columns\n",
    "    \n",
    "    categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
    "\n",
    "    if not encoder: #only scaler\n",
    "        transformers = [('num', RobustScaler(), ~dataset.columns.isin(categorical_columns))]\n",
    "    elif not scaler: #only encoder\n",
    "        transformers = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)]\n",
    "    elif not encoder and not scaler:\n",
    "        transformers = []\n",
    "    else: # both scaler and encoder\n",
    "        transformers=[\n",
    "            ('num', RobustScaler(), ~dataset.columns.isin(categorical_columns)),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ]\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # set parameters per KNN case\n",
    "    if isinstance(classifier, KNeighborsClassifier):\n",
    "        parameter_type = 'classifier__n_neighbors'\n",
    "        if parameters_grid_search:\n",
    "            parameters = {parameter_type: parameters_grid_search}  # n_neighbors values to be explored \n",
    "        else:\n",
    "            parameters = {parameter_type: [3, 5, 7, 9, 11]} \n",
    "\n",
    "        feature_selector_type = SelectKBest(score_func=f_classif)\n",
    "        selector_parameter = 'feature_selection__k'\n",
    "        \n",
    "    # set parameters per RF case\n",
    "    if isinstance(classifier, RandomForestClassifier):\n",
    "        parameter_type = 'classifier__n_estimators'\n",
    "        if parameters_grid_search:\n",
    "            parameters = {parameter_type: parameters_grid_search}  \n",
    "        else:\n",
    "            parameters = {parameter_type: [10, 50, 200, 500, 1000]} \n",
    "\n",
    "        feature_selector_type = RFE(estimator=classifier)\n",
    "        selector_parameter = 'feature_selection__n_features_to_select'\n",
    "\n",
    "    # set parameters per LinearSVC case\n",
    "    if isinstance(classifier, CalibratedClassifierCV):\n",
    "        parameter_type = 'classifier__estimator__C'\n",
    "        if parameters_grid_search:\n",
    "            parameters = {parameter_type: parameters_grid_search}  \n",
    "        else:\n",
    "            parameters = {parameter_type: [0.001, 0.01, 0.1, 1, 10, 100]}  \n",
    "\n",
    "        feature_selector_type = RFE(estimator=LinearSVC(dual=False, random_state=42))\n",
    "        selector_parameter = 'feature_selection__n_features_to_select'\n",
    "\n",
    "    \n",
    "    if feature_selector: # in case feature selection is required\n",
    "        \n",
    "        sample_transformed = preprocessor.fit_transform(dataset)\n",
    "        n_total_features = sample_transformed.shape[1]\n",
    "        print(\"Total quantity of features after one hot encoding :\" + str(n_total_features))\n",
    "        \n",
    "        # Pipeline with feature selector\n",
    "        pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', feature_selector_type),\n",
    "        ('classifier', classifier)  \n",
    "        ])\n",
    "        if parameters_feature_selector:\n",
    "            parameters.update({selector_parameter: parameters_feature_selector})\n",
    "        else: \n",
    "            parameters.update({selector_parameter: list(range(1, n_total_features+1))})\n",
    "            \n",
    "    else:\n",
    "        # Pipeline without feature selector\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', classifier),\n",
    "            ])\n",
    "\n",
    "    # allows to change k of cross validation from the outside\n",
    "    if cv:\n",
    "        k_folds = cv\n",
    "    else:\n",
    "        k_folds = 5\n",
    "\n",
    "    # Creating GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv= KFold(n_splits=k_folds, shuffle = True, random_state = 42), scoring=scoring, refit='accuracy')\n",
    "\n",
    "    # Executing \n",
    "    grid_search.fit(dataset, target)\n",
    "\n",
    "    # Extraction of results\n",
    "\n",
    "    means_accuracy = grid_search.cv_results_['mean_test_accuracy']\n",
    "    stds_accuracy = grid_search.cv_results_['std_test_accuracy']\n",
    "    means_precision = grid_search.cv_results_['mean_test_precision']\n",
    "    means_recall = grid_search.cv_results_['mean_test_recall']\n",
    "    means_f1_score = grid_search.cv_results_['mean_test_f1_score']\n",
    "    params = grid_search.cv_results_['params']\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_parameter = [grid_search.best_params_[parameter_type]]\n",
    "    best_accuracy = grid_search.best_score_\n",
    "    best_f1_score = grid_search.cv_results_['mean_test_f1_score'][grid_search.best_index_]\n",
    "    best_scores = [best_accuracy, best_f1_score]\n",
    "\n",
    "    feature_names = []\n",
    "    for feature in best_model.named_steps['preprocessor'].get_feature_names_out():\n",
    "        feature_names.append(feature)\n",
    "    # Visualization of results\n",
    "\n",
    "    if not parameters_grid_search: #the parameter is not choose from the outside\n",
    "        print(\"Cross-validation results for each combination of hyperparameters:\")\n",
    "        for mean_acc, std_acc, mean_prec, mean_rec, mean_f1, params in zip(means_accuracy, stds_accuracy, means_precision, means_recall, means_f1_score, params):\n",
    "            print(f\"Parameters: {params}, Accuracy: {mean_acc:.3f} (±{std_acc:.3f}), Precision: {mean_prec:.3f}, Recall: {mean_rec:.3f}, F1-score: {mean_f1:.3f}\")\n",
    "\n",
    "        print(\"\\nBest parameter founded:\")\n",
    "        print(best_parameter)\n",
    "        print(\"\\nThe scores achieved are:\")\n",
    "\n",
    "    print(\"Accuracy:\", best_accuracy)\n",
    "    print(\"F1-score:\", best_f1_score)\n",
    "\n",
    "    if feature_selector:\n",
    "        # Get the support mask and selected feature names from the preprocessed data\n",
    "        feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        selected_mask = best_model.named_steps['feature_selection'].get_support()\n",
    "        selected_features = []\n",
    "        for feature in feature_names[selected_mask]:\n",
    "            selected_features.append(feature)\n",
    "\n",
    "        print(\"\\nSelected features are:\")\n",
    "        print(f\"Number of selected features: {len(selected_features)}\")\n",
    "        print(\"Selected features:\", selected_features)\n",
    "\n",
    "        non_selected_mask = ~selected_mask\n",
    "        non_selected_features = feature_names[non_selected_mask]\n",
    "\n",
    "        print(\"\\nNon selected features are:\")\n",
    "        print(f\"Number of non selected features: {len(non_selected_features)}\")\n",
    "        print(\"Selected features:\", non_selected_features)\n",
    "\n",
    "        best_parameter.append(grid_search.best_params_[selector_parameter])\n",
    "\n",
    "        return best_model, best_parameter, best_scores, selected_features\n",
    "        \n",
    "    return best_model, best_parameter, best_scores, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start training our classifiers with the default values, so we can find the most accurate parameters for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NEAREST NEIGHBOOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the K-Nearest Neighbors, we need only numerical features, so we will use one-hot-encoding to turn the categorical features (which aren't ordinal) into numerical features. We also need to perform normalization, as we are using distancies to classify instancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "knn_model, knn_parameter, knn_scores, _= general_pipeline(X_SMOTE, Y_SMOTE, classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we seen, we found that the best accuracy of the model is achieved using k = 3 and we obtained accuracy = 0.913 and F1-score = 0.91."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the algorithm on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation on test data \n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy is lower 0.88 (vs 0.913), but the F1-score is now 0.912 (vs 0.91), which is a little bit higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better idea of the performance, we display also the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=['ASD', 'Control'], yticklabels=['ASD', 'Control'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in general we have a good fit, but there are many persons that are misclassified. In particular, there are 10 autistics that are classified as controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Random Forest classifier, I need to have only numerical features, so we will use one-hot-encoding to turn the categorical features (which aren't ordinal) into numerical features. It's not necessary to perform any normalization, but as we want to compare the performance of this classifier with others that use normalization, we will also use normalized data to train this classifier (it's not harmfull for the Random Forest Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "rf_model, rf_parameter, rf_scores, rf_features_names = general_pipeline(X_SMOTE, Y_SMOTE, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the best accuracy, using 200 trees, is 0.981, and equal to the best F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Model on Test Data \n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy decrease to 0.978 (vs 0.981), while the F1 score increases to 0.985 (vs 0.981), both are good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=['ASD', 'Control'], yticklabels=['ASD', 'Control'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordly to what we said before, there is an higher accuracy in the model, so we have a better fit of the model to the labeled data, with only 2 autistics misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still using the default parameters and both encoding and scaler.\n",
    "\n",
    " Note: we use the calibrated classifier cv on the LinearSVC to enable the possibility to compute the ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CalibratedClassifierCV(LinearSVC(dual=False, random_state=42))\n",
    "svc_model, svc_parameter, svc_scores, _ = general_pipeline(X_SMOTE, Y_SMOTE, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy encountered is 0.954 and F1-score is 0.952."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we didn't really perfomed the grid search, because this method is not sensitive to an external parameter to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check on the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation on test data\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case there is a decrease of accuracy using test data, going to 0.946 (vs 0.954), but we observe an increase in F1-score, going to 0.961 (vs 0.952)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualizzazione della matrice di confusione\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=['ASD', 'Control'], yticklabels=['ASD', 'Control'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this algorithm has a worst performance than RFC, but better than KNN, misclassifing 5 autistics as control. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Between Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better compare the classifiers we evaluate the area under the ROC curve per each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the AUC-ROC for a given model based on the predicted probabilities\n",
    "def evaluate_roc_auc(model, X_test, y_test):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "    return auc_roc\n",
    "\n",
    "knn_auc_roc = evaluate_roc_auc(knn_model, X_test, y_test)\n",
    "rf_auc_roc = evaluate_roc_auc(rf_model, X_test, y_test)\n",
    "svc_auc_roc = evaluate_roc_auc(svc_model, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"K-Nearest Neighbors Classifier AUC-ROC:\", knn_auc_roc)\n",
    "print(\"Random Forest Classifier AUC-ROC:\", rf_auc_roc)\n",
    "print(\"Support vector Classifier AUC-ROC:\", svc_auc_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the ROC curve for a given model\n",
    "def plot_roc_curve(model, X_test, y_test, model_name):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=model_name)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "# We need to make the diagnostic binary (instead of 1 and 2)\n",
    "y_test_binary = (y_test == 2).astype(int)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(knn_model, X_test, y_test_binary, 'K-Nearest Neighbors Classifier')\n",
    "plot_roc_curve(rf_model, X_test, y_test_binary, 'Random Forest Classifier')\n",
    "plot_roc_curve(svc_model, X_test, y_test_binary, 'Support vector Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that that the curve that gets more closer to one in the True Positive Rate is the one with the best performance. As we predicted before, the more accurate classifier for this problem is the RF (0.983 of auc score), followed by the SVC (0.973 of auc score) and finally by the KNN (0.954 of auc score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to check if the possible presence of outlier that we checked  during the exploration is really affecting the performance of our classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses Local Outlier Factor to determine if a subject can be considered as outlier or not, the contamination_factor is set by a grid search. It returns the datasets without outliers subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detector(dataset, diagnosis, contamination_factor, k_neighbors):\n",
    "    dataset_outliers = dataset.select_dtypes(include=[np.number])\n",
    "    X = dataset_outliers.values\n",
    "\n",
    "    # Initialize Local Outlier Factor\n",
    "    lof = LocalOutlierFactor(n_neighbors=k_neighbors, contamination=contamination_factor)  # Adjust parameters as needed\n",
    "    '''n_neighbors | if too small: model sensible to noise and random outliers\n",
    "                     if too large: diculties in local outliers detection, in particular if in absence of a uniform distribution\n",
    "       contamination | data portion expected as outliers'''\n",
    "    \n",
    "    # Fit the model and predict outliers\n",
    "    outliers = lof.fit_predict(X)\n",
    "\n",
    "    # Print number of detected outliers\n",
    "    print(f\"___________________\\nNumber of outliers detected: {np.sum(outliers == -1)}\")\n",
    "\n",
    "    # outliers == -1 indicates outliers, 1 indicates inliers\n",
    "    dataset_outliers['outlier'] = outliers\n",
    "\n",
    "    outlier_subjects = dataset_outliers[dataset_outliers['outlier'] == -1]\n",
    "\n",
    "    pd.set_option('display.max_columns', None); outlier_subjects.T\n",
    "\n",
    "    # Stores the datasets without outliers\n",
    "    \n",
    "    dataset_without_outliers = dataset[dataset_outliers['outlier'] == 1]\n",
    "         \n",
    "    diagnosis_without_outliers = diagnosis[dataset_outliers['outlier'] == 1]\n",
    "\n",
    "    return dataset_without_outliers, diagnosis_without_outliers, lof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the grid search to find the best contamination factor for each one of the classifiers and we check their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialized dictionaries to store best accuracy, best models, best parameters, contamination, and k_neighbors for each classifier.\n",
    "best_accuracies = {KNeighborsClassifier: 0, RandomForestClassifier: 0, CalibratedClassifierCV: 0}\n",
    "best_f1_scores = {KNeighborsClassifier: 0, RandomForestClassifier: 0, CalibratedClassifierCV: 0}\n",
    "best_models = {KNeighborsClassifier: 0, RandomForestClassifier: 0, CalibratedClassifierCV: 0}\n",
    "best_parameters = {KNeighborsClassifier: 0, RandomForestClassifier: 0, CalibratedClassifierCV: 0}\n",
    "best_contaminations = {KNeighborsClassifier: None, RandomForestClassifier: None, CalibratedClassifierCV: None}\n",
    "best_k_neighbors = {KNeighborsClassifier: None, RandomForestClassifier: None, CalibratedClassifierCV: None}\n",
    "\n",
    "# Initialized variables to store the best overall accuracy, f1-score, corresponding classifier, contamination factor, and k_neighbors.\n",
    "best_overall_accuracy = 0\n",
    "best_overall_f1_score = 0\n",
    "best_overall_classifier = None\n",
    "best_overall_contamination = None\n",
    "best_overall_k_neighbors = None\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'contamination_factor': [0.0033, 0.0098, 0.0230, 0.0361, 0.05, 0.081],\n",
    "    'k_neighbors': [2, 3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Iterate over all parameter combinations\n",
    "for params in grid:\n",
    "    contamination_factor = params['contamination_factor']\n",
    "    k_neighbors = params['k_neighbors']\n",
    "\n",
    "    ASD_phenotypic_without_outliers, ASD_diagnosis_without_outliers, _ = outlier_detector(ASD_phenotypic, ASD_diagnosis, contamination_factor, k_neighbors)\n",
    "\n",
    "    X_train_od, X_test_od, y_train_od, y_test_od = train_test_split(ASD_phenotypic_without_outliers, ASD_diagnosis_without_outliers['DX_GROUP'], test_size=0.3, random_state=42)\n",
    "\n",
    "    categorical_columns = X_train_od.select_dtypes(include=['object']).columns\n",
    "    categorical_features = categorical_columns.tolist()\n",
    "\n",
    "    # Initialize the SMOTE-NC object\n",
    "    sampler = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "    X_SMOTE_od, Y_SMOTE_od = sampler.fit_resample(X_train_od, y_train_od)\n",
    "\n",
    "    classifiers = [KNeighborsClassifier(), RandomForestClassifier(random_state=42), CalibratedClassifierCV(LinearSVC(dual=False, random_state=42))]\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        print(f\"\\n_____\\nEvaluating classifier: {type(classifier).__name__}\")\n",
    "        print(f\"with contamination factor = {contamination_factor} and k_neighbors = {k_neighbors}\")\n",
    "        \n",
    "        model, parameters, scores, _ = general_pipeline(X_SMOTE_od, Y_SMOTE_od, classifier)\n",
    "        \n",
    "        # Update best accuracy and contamination for the specific classifier\n",
    "        classifier_type = type(classifier)\n",
    "\n",
    "        if scores[0] > best_accuracies[classifier_type]:\n",
    "            best_accuracies[classifier_type] = scores[0]\n",
    "            best_f1_scores[classifier_type] = scores[1]\n",
    "            best_parameters[classifier_type] = parameters\n",
    "            best_contaminations[classifier_type] = contamination_factor\n",
    "            best_k_neighbors[classifier_type] = k_neighbors\n",
    "            best_models[classifier_type] = model\n",
    "        \n",
    "        # Update best overall accuracy and corresponding classifier, contamination, and k_neighbors\n",
    "        if scores[0] > best_overall_accuracy:\n",
    "            best_overall_accuracy = scores[0]\n",
    "            best_overall_f1_score = scores[1]\n",
    "            best_overall_classifier = classifier_type\n",
    "            best_overall_contamination = contamination_factor\n",
    "            best_overall_k_neighbors = k_neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we print the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best results for each classifier\n",
    "for classifier_type in best_accuracies:\n",
    "    print(f\"\\n{classifier_type.__name__} with best parameter = {best_parameters[classifier_type]}\")\n",
    "    print(f\"Best contamination factor: {best_contaminations[classifier_type]} with k_neighbors: {best_k_neighbors[classifier_type]}\")\n",
    "    print(f\"Achieved Accuracy : {best_accuracies[classifier_type]}, F1_score: {best_f1_scores[classifier_type]}\")\n",
    "\n",
    "# Print the best overall result\n",
    "print(f\"\\nBest overall contamination factor: {best_overall_contamination} with k_neighbors: {best_overall_k_neighbors} for classifier: {best_overall_classifier.__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the following outcomes for the training:\n",
    "- Best contamination factor for KNeighborsClassifier: 0.0361 with accuracy: 0.942 and f1_score: 0.935\n",
    "- Best contamination factor for RandomForestClassifier: 0.0361 with accuracy: 0.994 and f1_score: 0.993\n",
    "- Best contamination factor for CalibratedClassifierCV: 0.0361 with accuracy: 0.968 and f1_score: 0.966\n",
    "\n",
    "Highlighting that RandomForestClassifier is one more time the most accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a visual idea of the eliminated outliers, we perfomed a PCA transformation of the space to 2 dimensions for 0.0361 contamination factor and 3 k neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_without_outliers, ASD_diagnosis_without_outliers, lof = outlier_detector(ASD_phenotypic, ASD_diagnosis, best_overall_contamination, best_overall_k_neighbors)\n",
    "X_train_od, X_test_od, y_train_od, y_test_od = train_test_split(ASD_phenotypic_without_outliers, ASD_diagnosis_without_outliers['DX_GROUP'], test_size=0.3, random_state=42)\n",
    "\n",
    "num_features = ASD_phenotypic.select_dtypes(np.number)\n",
    "X = num_features.values\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "outliers = X_reduced[ASD_phenotypic_without_outliers.index]\n",
    "outlier_indices = ASD_phenotypic.index.difference(ASD_phenotypic_without_outliers.index)\n",
    "outliers = X_reduced[outlier_indices]\n",
    "\n",
    "lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "# Plot the data points with color indicating LOF scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=-lof_scores, cmap='hot_r', edgecolor='k', label='Inliers')\n",
    "plt.colorbar(label='LOF Score')\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], c='blue', edgecolor='k', marker='x', s=100, label='Outliers')  # Highlight outliers\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Data points with LOF scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dots with higher LOF scores were eliminated. Some of them are placed peripherical, but others are in a dense region (this could happen due to the 2d transformation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we check the accuracies on the test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "model = best_models[type(KNeighborsClassifier())]\n",
    "y_pred = model.predict(X_test_od)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test_od, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter selected per k is 3.\n",
    "\n",
    "Previous results were:\n",
    "- Accuracy with outliers - 0.88\n",
    "- F1-score with outliers - 0.912\n",
    "\n",
    "And now we have:\n",
    "- Accuracy without outliers - 0.809\n",
    "- F1-score without outliers - 0.862\n",
    "\n",
    "So it seems that the elimination of the outliers is not benefitial for the classificator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "model = best_models[type(RandomForestClassifier(random_state=42))]\n",
    "y_pred = model.predict(X_test_od)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test_od, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter selected for the amount of trees is 50.\n",
    "\n",
    "Previous results were:\n",
    "- Accuracy with outliers - 0.978\n",
    "- F1-score with outliers - 0.985\n",
    "\n",
    "And now we have:\n",
    "- Accuracy without outliers - 0.888\n",
    "- F1-score without outliers - 0.921\n",
    "\n",
    "We can see that the accuracy and f1-score are lower, so maybe is better to work in the dataset without outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "model = best_models[type(CalibratedClassifierCV(LinearSVC(dual=False, random_state=42)))]\n",
    "y_pred = model.predict(X_test_od)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test_od, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results were:\n",
    "- Accuracy with outliers - 0.946\n",
    "- F1-score with outliers - 0.961\n",
    "\n",
    "And now we have:\n",
    "- Accuracy without outliers - 0.843\n",
    "- F1-score without outliers - 0.883\n",
    "\n",
    "Also in this case we achieve a worst performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know if the features that we are using are relevant for the training of the algorithm, we perform some feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K_NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the KNN we use SelectKBest algorithm and we maintain the k encountered in the version with \"outliers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "fs_KNN, fs_KNN_parameter, fs_KNN_scores, fs_KNN_selected_features = general_pipeline(X_SMOTE, Y_SMOTE, classifier, parameters_grid_search = knn_parameter, feature_selector=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the most important features are ADI_R_VERBAL_TOTAL_BV, ADOS_TOTAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "y_pred = fs_KNN.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results were:\n",
    "- Accuracy with outliers - 0.88\n",
    "- F1-score with outliers - 0.912\n",
    "\n",
    "And now we have:\n",
    "- Accuracy with feature selection - 0.935\n",
    "- F1-score without feature selection - 0.953\n",
    "\n",
    "The results are way better. This means that for KNN is enough to have ADI_R_VERBAL and ADOS_TOTAL to discriminate if a subject has autism disorder or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the RF we use the Recursive Feature Elimination and the quantity of trees is set by the best one achieved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "fs_RF, fs_RF_parameter, fs_RF_scores, fs_RF_selected_features = general_pipeline(X_SMOTE, Y_SMOTE, classifier, parameters_grid_search = rf_parameter, feature_selector=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the most important features are ADI_R_VERBAL_TOTAL_BV and ADOS_TOTAL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "y_pred = fs_RF.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results were:\n",
    "- Accuracy with outliers - 0.978\n",
    "- F1-score with outliers - 0.985\n",
    "\n",
    "And now we have:\n",
    "- Accuracy without outliers - 0.935\n",
    "- F1-score without outliers - 0.954\n",
    "\n",
    "In this case the accuracy and the f1-score decrease, so in the reality the performance is worst excluding the non-selected feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear support vector machine uses RFE as well to select the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CalibratedClassifierCV(LinearSVC(dual=False, random_state=42))\n",
    "\n",
    "fs_SVM, fs_SVM_parameter, fs_SVM_scores, fs_SVM_selected_features = general_pipeline(X_SMOTE, Y_SMOTE, classifier,  parameters_grid_search = svc_parameter, feature_selector=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the selected features were ADI_R_VERBAL_TOTAL_BV, ADOS_TOTAL, PIQ_TEST_TYPE_DAS, and PIQ_TEST_TYPE_WISC. We observe that FIQ, PIQ and VIQ are not present, but their subtypes are important, which is a strange behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "y_pred = fs_SVM.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results were:\n",
    "- Accuracy with outliers - 0.946\n",
    "- F1-score with outliers - 0.961\n",
    "\n",
    "And now we have:\n",
    "- Accuracy without outliers - 0.935\n",
    "- F1-score without outliers - 0.953\n",
    "\n",
    "In this case the accuracy and the f1-score are exactly the same. In this way we can work with only the selected features without worries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ROC curves, we compare now the best models, so the one with feature selection for knn (no outlier detection) and the ones without future selection for RF and SVC (no outlier detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fs_auc_roc = evaluate_roc_auc(fs_KNN, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"K-Nearest Neighbors Classifier AUC-ROC:\", knn_fs_auc_roc)\n",
    "print(\"Random Forest Classifier AUC-ROC:\", rf_auc_roc)\n",
    "print(\"Support vector Classifier AUC-ROC:\", svc_auc_roc)\n",
    "\n",
    "# Plot ROC curve for both models\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fs_KNN, X_test, y_test_binary, 'K-Nearest Neighbors Classifier')\n",
    "plot_roc_curve(rf_model, X_test, y_test_binary, 'Random Forest Classifier')\n",
    "plot_roc_curve(svc_model, X_test, y_test_binary, 'Support vector Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more accurate classifier for this problem is the RF:\n",
    "- previous ROC score was 0.983, now is 0.991 (so better fit)\n",
    "\n",
    "Followed by the KNN:\n",
    "- previous ROC score was 0.954, now is 0.987 (so better fit)\n",
    "\n",
    "Finally by the SVC:\n",
    "- previous ROC score was 0.973, now is 0.984 (so better fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the random tree decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen that RF is the best classifier, we would like to see if we can extract interesting information from the decision mechanisms that it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trees = rf_parameter[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as many trees are created using random selection of the features to split, we haven't a way to see a good tree partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your feature and target names\n",
    "feature_names = rf_features_names\n",
    "\n",
    "target_names = [\"Autistic\", \"Control\"]  # Adjust according to your target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = rf_model.named_steps['preprocessor']\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=transformer.get_feature_names_out())\n",
    "X_test_transformed = X_test_transformed_df[feature_names]\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=transformer.get_feature_names_out())\n",
    "X_train_transformed = X_train_transformed_df[feature_names]\n",
    "\n",
    "# Access the random forest classifier from the pipeline\n",
    "rf = rf_model.named_steps['classifier']\n",
    "\n",
    "accuracies = []\n",
    "for i, tree in enumerate(rf.estimators_):\n",
    "    # Make predictions with the individual tree\n",
    "    y_pred = tree.predict(X_test_transformed.values)\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append((i, accuracy))\n",
    "\n",
    "# Find the tree with the highest accuracy\n",
    "most_accurate_tree_index, highest_accuracy = max(accuracies, key=lambda x: x[1])\n",
    "most_accurate_tree = rf.estimators_[most_accurate_tree_index]\n",
    "\n",
    "print(f\"The most accurate tree is tree number {most_accurate_tree_index} with accuracy of {highest_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we saw that the tree that better classify the test data has a accuracy of only 0.11. So it's not reliable to learn rules from there.\n",
    "\n",
    "However, we will show it, to have an idea of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export one of the trees from the forest\n",
    "tree = rf.estimators_[most_accurate_tree_index]\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(200,100))\n",
    "plot_tree(most_accurate_tree, feature_names=feature_names, class_names=target_names, filled=True, rounded=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the accuracy of the tree is not good, so this splits are thresholds are not reliables.\n",
    "\n",
    "Another usefull insight can be achieved seeing the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range((X_train_transformed.shape[1])):\n",
    "    print(f\"{f + 1}. feature {indices[f]} : {feature_names[f]} ({importances[indices[f]]})\")\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train_transformed.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train_transformed.shape[1]), X_train_transformed.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X_train_transformed.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the more relevant feature for the decision is ADI_R_VERBAL_TOTAL_BV, followed by ADOS_TOTAL. This outcome seems to be meaningfull due to the fact that this two feature are the only one that are really created to account autism. But we can't overcome the fact that they are also the ones that has the biggest quantity of missing values, so we have to be carefull with the analysis of this results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the dataset gave by the authors which has no presence of missing values and has been treated with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_literature = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_phenotypic_paper.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_literature.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the columns that we don't need and separate data for the diagnosis database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_clinical_lit = ASD_phenotypic_literature[['DX_GROUP']]\n",
    "# Drop  columns DX_GROUP and storage it apart \n",
    "ASD_phenotypic_lit = ASD_phenotypic_literature.drop(columns=['DX_GROUP', 'DSM_IV_TR', 'Unnamed: 0.1', 'Unnamed: 0', 'SUB_ID', 'X', 'subject', 'SITE_ID', 'FILE_ID', 'EYE_STATUS_AT_SCAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_lit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_clinical_lit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset and apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lit, X_test_lit, y_train_lit, y_test_lit = train_test_split(ASD_phenotypic_lit, ASD_clinical_lit['DX_GROUP'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the SMOTE-NC object\n",
    "sampler = SMOTE(random_state=42)\n",
    "X_SMOTE_lit, Y_SMOTE_lit = sampler.fit_resample(X_train_lit, y_train_lit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to perform the Random Forest Classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "rf_model_lit, rf_parameter_lit, rf_scores_lit, _ = general_pipeline(X_SMOTE_lit, Y_SMOTE_lit, classifier,  parameters_grid_search = rf_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit on the test to see the final accuracy and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model_lit.predict(X_test_lit)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test_lit, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
