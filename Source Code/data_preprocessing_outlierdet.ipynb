{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ABIDE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays the Autism Spectrum Disorder is an hot topic in research.\n",
    "The fondamental issue to be assessed is the absence of a Gold Standard methodology for diagnosis evaluation: based on clinical interviews and behavioral assessments.\n",
    "\n",
    "In order to improve knowledge and discover rules behind ASD, ABIDE Dataset has been borned.\n",
    "\n",
    "The ABIDE Dataset is a collaborative dataset, multi-sites, containing several kinds of informations: fMRI, sMRi, phenotypic, diagnosis.\n",
    "\n",
    "This project has been focused on Phenotypic characteristics dealing with the fondamental issue mentioned above.\n",
    "\n",
    "The ABIDE Dataset has been validated in several papers.\n",
    "Our reference and starting poin is available here:\n",
    "https://pubmed.ncbi.nlm.nih.gov/30393630/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #to interact to the file system\n",
    "import numpy as np #Statistics\n",
    "import pandas as pd #Database Technology <-> Data preproc & Data Analysis\n",
    "from matplotlib import pyplot as plt #Visualization\n",
    "import seaborn as sns #Visualization\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import RobustScaler #scikit-learn -> ML\n",
    "import OurFunctions as of #saperated collection\n",
    "import Optimization as opt\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_phenotypic.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 1112 subjects and has 74 features.\n",
    "\n",
    " At first look, it is noticeable the presence of categorical and numerical features and missing values. Moreover, we can see the presence of the categories DX_GROUP and DSM_IV_TR, that are described from the ABIDE dataset legend as diagnostic, so we will further remove them from the dataset for the model predictor construction. \n",
    " We can also proceed deleting the feature EYE_STATUS_AT_SCAN, because it deals with the eyes opening by the subject  during the fMRI, thus it is meaningless for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DX_GROUP and DSM_IV_TR are our targets.\n",
    "DX_GROUP contains info about ASD detection (yes/no). \n",
    "DSM_IV_TR specifyes which kind of autism, if there is.\n",
    "However, in our investigation, we are interested only in evaluating the presence or not of the disorder, so we will not consider the information in DSM_IV_TR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the balancing of our dataset, we check the target feature: 'DX_GROUP'. \n",
    "\n",
    "1 | Autism detected\n",
    "\n",
    "2 | Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.evaluate_balancing(ASD_phenotypic_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DX_GROUP is balanced enought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to have a better view of the information contained in the dataset we display the names of the features and the respective types and quantity of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a huge amount of missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a better view of the distribution of the null values, we check the presence of missing values catalogated as None or numpy.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the missing values\n",
    "\n",
    "ASD_phenotypic_original, percent_missing = of.count_missing_value(ASD_phenotypic_original)\n",
    "\n",
    "# We implemented a function \"select_columns\", that is able to define wich columns are numerical\n",
    "# and which ones are categorical (also redefine the objects as categorical in the dataset)\n",
    "numeric_columns, categorical_columns, ASD_phenotypic_original = of.select_columns(ASD_phenotypic_original)\n",
    "\n",
    "# We plot the distribution of missing values, with the specification of numeric and categorical columns\n",
    "of.plot_missing_values(percent_missing, numeric_columns, legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the majority of the features the amount of missing values is not depreciable, so we can say that the information that is stored in the feature is not enough to create a reliable classier/cluster algortihm based on it. The same comment can be done for the subjects. We need to work on a dataset that has a maximum of 10% of missing values per feature, so we need to clean this in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look on the general statistics for the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the presence of \"-9999\" as minimum value for different features, a value that is commonly used to denote missing data or values out of range, so it should be better to consider them as NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an overall view of the dataset, let's start to work on it in order to clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we decided to apart the feature DX_GROUP that give the diagnosis of the subjects, as our control label. As written above, DSM_IV_TR it is irrelevant for our purpose.\n",
    "The same for EYE_STATUS_AT_SCAN. These two features are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ASD_phenotypic = ASD_phenotypic_original.drop(columns=['DSM_IV_TR','EYE_STATUS_AT_SCAN'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we decide to drop SUB_ID, as it only store the information about the ID of the subject. But before doing this, we check if there aren't replicated subjects. Then, since we don't find any duplicate, we simply drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicate values in the 'SUB_ID' column\n",
    "duplicate_ids = ASD_phenotypic['SUB_ID'].duplicated(keep=False)\n",
    "\n",
    "# Get the unique duplicate IDs\n",
    "unique_duplicate_ids = ASD_phenotypic.loc[duplicate_ids, 'SUB_ID'].unique()\n",
    "\n",
    "#Drop column if there aren't duplicates\n",
    "if len(unique_duplicate_ids) == 0:\n",
    "    ASD_phenotypic = ASD_phenotypic.drop(columns=['SUB_ID'])\n",
    "    print(\"SUB_ID has been dropped\")\n",
    "else:\n",
    "    print(\"There are replicated values:\" + str(unique_duplicate_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a real count of the amount of missing values per feature, we change the -9999 values present in the overall data to np.NaN (we can make this because we know from the datasheets that -9999 is a value out of range for all the features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ASD_phenotypic:\n",
    "    \n",
    "    # Replace -9999 and \"-9999\" with NaN\n",
    "    ASD_phenotypic[column] = ASD_phenotypic[column].replace(['-9999', -9999], np.NaN)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that there are two features ADI_R_RSRCH_RELIABLE and ADOS_RSRCH_RELIABLE: these are relating the personal that took the tests, indicating if he/she is a trained professional or not.\n",
    "We use this information to evaluate the reliability of the score-tests. In order to avoid the inclusion of not reliable informations, we decide to eliminate the subjects that has a 0 in both categories (not reliable), while if the value is 1 or missed, we maintain the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for rows to keep\n",
    "filter = (ASD_phenotypic['ADI_R_RSRCH_RELIABLE'] != 0) | (ASD_phenotypic['ADOS_RSRCH_RELIABLE'] != 0)\n",
    "\n",
    "# Calculate the number of subjects to delete\n",
    "deleted_subjects = len(ASD_phenotypic) - filter.sum()\n",
    "\n",
    "# Apply the mask to both DataFrames\n",
    "ASD_phenotypic= ASD_phenotypic[filter]\n",
    "\n",
    "# Print the number of subjects deleted\n",
    "print(\"Number of subjects deleted:\", deleted_subjects)\n",
    "\n",
    "ASD_phenotypic = ASD_phenotypic.drop(columns=['ADI_R_RSRCH_RELIABLE','ADOS_RSRCH_RELIABLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the cleaning on our starting dataset:\n",
    "\n",
    "- Displaying the distribution of missing values\n",
    "- Menaging our dataset with the purpose to achieve at a cleaned one with two conditions:\n",
    "    \n",
    "    - 10 % of NaN per feature\n",
    "    - at least 1/4 of subjects involved\n",
    "    - balancing of dataset (max 70/30 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizzazione dei missing values\n",
    "msno.matrix(ASD_phenotypic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our aim, following the pipeline of reference paper, we create an alghoritm with boundary contitions already mentioned. Moreover, we force the presence of key features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Features to mantain\n",
    "key_features = ['FIQ', 'VIQ', 'PIQ', 'ADI_R_VERBAL_TOTAL_BV', 'ADOS_TOTAL']\n",
    "\n",
    "\n",
    "# Clining function on dataset\n",
    "ASD_phenotypic_cleaned = opt.remove_high_missing(ASD_phenotypic, key_features, balance_column='DX_GROUP', min_subjects=200, max_missing_percentage=10)\n",
    "\n",
    "# Print DataFrame cleaned \n",
    "ASD_phenotypic_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Missing Values for features and subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_cleaned.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the missing values\n",
    "ASD_phenotypic_cleaned, percent_missing = of.count_missing_value(ASD_phenotypic_cleaned)\n",
    "numeric_columns, categorical_columns, ASD_phenotypic_cleaned = of.select_columns(ASD_phenotypic_cleaned)\n",
    "\n",
    "of.plot_missing_values(percent_missing, numeric_columns, legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of missing value for subject instead of for feature\n",
    "nan_values_per_subject = ASD_phenotypic_cleaned.T.isna().sum()\n",
    "\n",
    "# Ording of missing values\n",
    "subjects_with_nan_sorted = nan_values_per_subject.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "of.plot_missing_values(subjects_with_nan_sorted, nan_values_per_subject, legend=False)\n",
    "plt.ylabel('Subjects')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.evaluate_balancing(ASD_phenotypic_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal has been achieved in part. Since that we want to mantain ADI_R_VERBAL_TOTAL_BV', 'ADOS_TOTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect features\n",
    "features_to_check = ['ADI_R_VERBAL_TOTAL_BV', 'ADOS_TOTAL']\n",
    "\n",
    "# Evaluate number of subjects with missing values for DX_GROUP\n",
    "missing_counts = ASD_phenotypic_cleaned[features_to_check + ['DX_GROUP']].isna().groupby(ASD_phenotypic_cleaned['DX_GROUP']).sum()\n",
    "\n",
    "# Evaluate total number of subjects for each DX_GROUP\n",
    "total_counts = ASD_phenotypic_cleaned['DX_GROUP'].value_counts()\n",
    "\n",
    "# Print results\n",
    "missing_counts_df = pd.DataFrame({\n",
    "    'Missing_ADI_R_VERBAL_TOTAL_BV': missing_counts['ADI_R_VERBAL_TOTAL_BV'],\n",
    "    'Missing_ADOS_TOTAL': missing_counts['ADOS_TOTAL'],\n",
    "    'Total_Subjects': total_counts\n",
    "})\n",
    "\n",
    "print(missing_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters out autistic subjects (DX_GROUP = 1) with missing values in the specified features\n",
    "missing_autistic = ASD_phenotypic_cleaned[(ASD_phenotypic_cleaned['DX_GROUP'] == 1) & (ASD_phenotypic_cleaned[features_to_check].isna().any(axis=1))]\n",
    "\n",
    "# Filters non-autistic subjects (DX_GROUP = 2) with missing values in the specified features\n",
    "missing_non_autistic = ASD_phenotypic_cleaned[(ASD_phenotypic_cleaned['DX_GROUP'] == 2) & (ASD_phenotypic_cleaned[features_to_check].isna().any(axis=1))]\n",
    "# Ottieni gli indici dei soggetti autistici con valori mancanti\n",
    "indices_to_drop = missing_autistic.index\n",
    "\n",
    "# Rimuovi i soggetti autistici dal DataFrame ASD_phenotypic_cleaned\n",
    "ASD_phenotypic_cleaned = ASD_phenotypic_cleaned.drop(index=indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.evaluate_balancing(ASD_phenotypic_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definisci il valore soglia del 25% per i valori mancanti\n",
    "desired_missing_percentage = 25\n",
    "\n",
    "\n",
    "cleaned_df = opt.optimization_rules(ASD_phenotypic_cleaned,features_to_check,desired_missing_percentage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the missing values\n",
    "\n",
    "cleaned_df,percent_missing = of.count_missing_value(cleaned_df)\n",
    "\n",
    "numeric_columns, categorical_columns, cleaned_df = of.select_columns(cleaned_df)\n",
    "\n",
    "of.plot_missing_values(percent_missing, numeric_columns, legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.evaluate_balancing(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store them in a new dataset called ASD_clinical\n",
    "ASD_clinical = cleaned_df[['DX_GROUP']]\n",
    "# Drop  columns DX_GROUP and storage it apart \n",
    "ASD_phenotypic = cleaned_df.drop(columns=['DX_GROUP'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to understand how our features are distributed, in order to know which kind of normalization of the data is more suitable and if we may need to proceed with outlier detection in classification phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of features\n",
    "of.plot_distributions(ASD_phenotypic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disucussing graph's results:\n",
    "We notice the presence of possible outliers per feature. Thus, we quantify the fraction in the entire dataset, in order to take in consideration this element in our future analysis (Outlier detection in classification). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'AGE_AT_SCAN': [{'threshold': 30, 'rule': 'greater'}],\n",
    "    'FIQ': [\n",
    "        {'threshold': 145, 'rule': 'greater'},\n",
    "        {'threshold': 70, 'rule': 'less'}\n",
    "    ],\n",
    "     'VIQ': [\n",
    "        {'threshold': 145, 'rule': 'greater'},\n",
    "        {'threshold': 70, 'rule': 'less'}\n",
    "    ],\n",
    "    \n",
    "    'PIQ': [\n",
    "        {'threshold': 140, 'rule': 'greater'},\n",
    "        {'threshold': 70, 'rule': 'less'}\n",
    "    ],\n",
    "    'ADI_R_VERBAL_TOTAL_BV': [{'threshold': 6, 'rule': 'less'}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of outliers per feature\n",
    "outlier_counts = {}\n",
    "total_samples = len(ASD_phenotypic)\n",
    "\n",
    "for feature, rules in thresholds.items():\n",
    "    outlier_count = 0\n",
    "    for rule in rules:\n",
    "        threshold = rule['threshold']\n",
    "        comparison = rule['rule']\n",
    "        if comparison == 'greater':\n",
    "            outlier_count += (ASD_phenotypic[feature] > threshold).sum()\n",
    "        elif comparison == 'less':\n",
    "            outlier_count += (ASD_phenotypic[feature] < threshold).sum()\n",
    "    outlier_counts[feature] = outlier_count\n",
    "\n",
    "# % outliers per feature\n",
    "outlier_percentages = {feature: (count / total_samples) * 100 for feature, count in outlier_counts.items()}\n",
    "\n",
    "# Print results\n",
    "for feature, percentage in outlier_percentages.items():\n",
    "    print(f'Feature: {feature}, Outlier Percentage: {percentage:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to procced in our data exploration, we want to perform Correlation Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the data to make comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns selection\n",
    "numeric_columns = ASD_phenotypic.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_columns = ASD_phenotypic.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "# Inizialization of StandardScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit transorm numerical data\n",
    "scaled_numeric_data = scaler.fit_transform(numeric_columns)\n",
    "\n",
    "# numerical features normalized\n",
    "numeric_columns_normalized = pd.DataFrame(scaled_numeric_data, columns=numeric_columns.columns, index=ASD_phenotypic.index)\n",
    "\n",
    "# New normalized DataFrame\n",
    "ASD_phenotypic_normalized = pd.concat([numeric_columns_normalized, categorical_columns], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_normalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_normalized = ASD_phenotypic_normalized.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Correlation Matrix\n",
    "correlation_matrix = numeric_normalized.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = numeric_normalized.corr()\n",
    "numeric_normalized.T\n",
    "f,ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(numeric_normalized.corr(), \n",
    "            annot=True, \n",
    "            linewidths=.5, \n",
    "            fmt= '.2f',\n",
    "            ax=ax,\n",
    "            vmin=-1, \n",
    "            vmax=1,\n",
    "            cmap = \"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns, categorical_columns, ASD_phenotypic_normalized = of.select_columns(ASD_phenotypic_normalized)\n",
    "# Compute Cramer's V for every pair of features\n",
    "cramer_v_scores = pd.DataFrame(index=categorical_columns, columns=categorical_columns)\n",
    "for feature1 in categorical_columns:\n",
    "    for feature2 in categorical_columns:\n",
    "        cramer_v = of.cramers_v(ASD_phenotypic_normalized[feature1], ASD_phenotypic_normalized[feature2])\n",
    "        cramer_v_scores.loc[feature1, feature2] = cramer_v\n",
    "\n",
    "# Plot heatmap of Cramer's V scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramer_v_scores.astype(float), \n",
    "            annot=True, \n",
    "            linewidths=.5, \n",
    "            fmt='.2f',\n",
    "            cmap=\"coolwarm\")\n",
    "plt.title(\"Cramer's V scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the information contained in FIQ_TEST_TYPE, VIQ_TEST_TYPE and PIQ_TEST_TYPE is almost the same, so it's reasonable thinking to eliminate from our analysis some of them.\n",
    "In particular, since the relation between them:\n",
    "Test Type\n",
    "- PIQ-FIQ = 1\n",
    "- VIQ-FIQ = 1\n",
    "- PIQ-VIQ = 1\n",
    "Thus, we procced keeping in consideration only PIQ_Test_Type (Performance Intelligence Quotient)  also for the presence of Ravens test type, that has a significant use in ASD investigation. The others test_type cosidarated relevant are shared between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_normalized = ASD_phenotypic_normalized.drop(columns=['VIQ_TEST_TYPE', 'FIQ_TEST_TYPE'])\n",
    "ASD_phenotypic = ASD_phenotypic.drop(columns=['VIQ_TEST_TYPE', 'FIQ_TEST_TYPE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual \"correlation\" between Categorical and Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_columns, _, _ = of.select_columns(ASD_phenotypic)\n",
    "\n",
    "# Definire le due feature categoriche specifiche in ordine desiderato\n",
    "categorical_columns = ['SITE_ID', 'PIQ_TEST_TYPE']\n",
    "\n",
    "# Number of plots per row\n",
    "plots_per_row = 4\n",
    "num_plots = len(numeric_columns) * len(categorical_columns)\n",
    "num_rows = (num_plots + plots_per_row - 1) // plots_per_row  # Calculate the number of rows needed\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(20, num_rows * 5))\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for cat_col in categorical_columns:\n",
    "    for numeric_col in numeric_columns:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        sns.boxplot(x=cat_col, y=numeric_col, data=ASD_phenotypic,\n",
    "                    order=ASD_phenotypic_normalized.groupby(cat_col, observed=False)[numeric_col].median().sort_values().index, ax=ax)\n",
    "        \n",
    "        # Highlight the median in red\n",
    "        median = ASD_phenotypic.groupby(cat_col, observed=False)[numeric_col].median().sort_values()\n",
    "        for i in range(len(median)):\n",
    "            ax.plot(i, median.iloc[i], 'ro')\n",
    "        \n",
    "        ax.set_title(f'Boxplot of {numeric_col} by {cat_col}')\n",
    "        ax.set_xlabel(cat_col)\n",
    "        ax.set_ylabel(numeric_col)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(plot_idx, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that it could be possible dealing with a Site-Specific or Test-Specific Analysis.\n",
    "However, at the moment, it is out of scope. We leave the possibility to proceed on these different lines in further investigations.\n",
    "Thus, we may decide to drop the feature SITE_ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, in order to avoid mismatch between attributes that are the same, but written with upper or lower characters, we decide to unify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make all the caracters upper for all the categorical features\n",
    "category_columns_upper = ASD_phenotypic.select_dtypes(include='category').apply(lambda x: x.str.upper())\n",
    "\n",
    "#We now modify them in the dataset\n",
    "ASD_phenotypic[category_columns_upper.columns] = category_columns_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns, categorical_columns, ASD_phenotypic = of.select_columns(ASD_phenotypic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain the names of the features \n",
    "categorical_column_names = categorical_columns.tolist()\n",
    "categorical_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SITE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SITE_ID refers to the place where the data have benne collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesso a una specifica colonna categorica utilizzando la lista di nomi\n",
    "specific_category_column = ASD_phenotypic[categorical_column_names[0]].value_counts(dropna=False)\n",
    "specific_category_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is data that has been collected from the same center that we decide to unify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to replace the categories for the indicated cases\n",
    "\n",
    "def replace_categories(category):\n",
    "    if \"UCLA\" in category:\n",
    "        return \"UCLA\"\n",
    "    if \"LEUVEN\" in category:\n",
    "        return \"LEUVEN\"\n",
    "    if \"UM\" in category:\n",
    "        return \"UM\"\n",
    "    else:\n",
    "        return category\n",
    "\n",
    "# Then we apply the replace function\n",
    "ASD_phenotypic[categorical_column_names[0]] = ASD_phenotypic[categorical_column_names[0]].apply(replace_categories).astype('category')\n",
    "\n",
    "# Now we check the new order\n",
    "specific_category_column = ASD_phenotypic[categorical_column_names[0]].value_counts(dropna=False)\n",
    "specific_category_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIQ_TEST_TYPE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIQ_TEST_TYPE refer to the type of test that each center chose to use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_category_column = ASD_phenotypic[categorical_column_names[1]].value_counts(dropna=False)\n",
    "specific_category_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to replace the categories for the indicated cases\n",
    "\n",
    "def replace_categories(category):\n",
    "    if pd.isna(category):  # Controlla se il valore è NaN\n",
    "        return category  # Se è NaN, restituisci lo stesso valore\n",
    "    if \"WASI\" in category:\n",
    "        return \"WASI\"\n",
    "    if \"WISC\" in category:\n",
    "        return \"WISC\"\n",
    "    if \"WAIS\" in category:\n",
    "        return \"WAIS\"\n",
    "    if \"DAS\" in category:\n",
    "        return \"DAS\"\n",
    "    if \"HAWIK\" in category:\n",
    "        return \"HAWIK\"\n",
    "    if \"PPVT\" in category:\n",
    "        return \"PPVT\"\n",
    "    if \"RAVENS\" in category:\n",
    "        return \"RAVENS\"\n",
    "   \n",
    "    else:\n",
    "        return category\n",
    "\n",
    "\n",
    "ASD_phenotypic[categorical_column_names[1]] = ASD_phenotypic[categorical_column_names[1]].apply(replace_categories).astype('category')\n",
    "specific_category_column = ASD_phenotypic[categorical_column_names[1]].value_counts(dropna=False)\n",
    "print(specific_category_column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to fullfill the missing values for all the features, based on an analysis of the information delivered by each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQ Test Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use features FIQ, VIQ and PIQ in order to fill some values in PIQ-TEST-TYPE.\n",
    "Since the presence of more missing values in \"Type\" features, we make a comparison for each couple of features. For instance: if for PIQ there is a value and for PIQ-TEST-TYPE there is a missing one, we fill it with the MODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features pairs to be checked \n",
    "feature_pairs = [\n",
    "    ('FIQ_TEST_TYPE', 'FIQ'),\n",
    "    ('PIQ_TEST_TYPE', 'PIQ'),\n",
    "    ('VIQ_TEST_TYPE', 'VIQ')]\n",
    "\n",
    "# Itering on each pair of features\n",
    "for test_type_col, score_col in feature_pairs:\n",
    "    if test_type_col not in ASD_phenotypic.columns:\n",
    "        test_type_df = ASD_phenotypic_original\n",
    "    else:\n",
    "        test_type_df = ASD_phenotypic\n",
    "    # Itering on each row DataFrame\n",
    "    for index, row in test_type_df.iterrows():\n",
    "        # Check if the value of 'test_type_col' is missed\n",
    "        if pd.isnull(row[test_type_col]):\n",
    "            # If the value of column 'score_col' is present\n",
    "            if not pd.isnull(row[score_col]):\n",
    "                # Calculate mode of 'test_type_col'\n",
    "                mode_test_type = test_type_df[test_type_col].mode()[0]\n",
    "                # Substituing 'test_type_col' missing value with the mode \n",
    "                test_type_df.at[index, test_type_col] = mode_test_type\n",
    "            # If both values in 'test_type_col' e 'score_col' are missing\n",
    "            elif pd.isnull(row[score_col]):\n",
    "                # Verify if is \"NOT_AVAILABLE\" is already present between categories of columns\n",
    "                if \"NOT_AVAILABLE\" not in test_type_df[test_type_col].cat.categories:\n",
    "                    # Add \"NOT_AVAILABLE\" as new category\n",
    "                    test_type_df[test_type_col] = test_type_df[test_type_col].cat.add_categories(\"NOT_AVAILABLE\")\n",
    "                # Assigning category 'NOT_AVAILABLE' to 'test_type_col'\n",
    "                test_type_df.at[index, test_type_col] = 'NOT_AVAILABLE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to fill the missing values, we note that as the data for the variables FIQ, VIQ, PIQ was obtained with different tests, there are also different scales for the scores to take into account. In this way, we prefer to apply a standardization so we have all the score on the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic_copy = ASD_phenotypic.copy()\n",
    "ASD_phenotypic_copy['SUB_ID'] = ASD_phenotypic_original['SUB_ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For FIQ, the score scale is between 30-170 if the test taken was \"DAS\", otherwise is 50-160.\n",
    "#We will unify all the data to the larger scale, i.e. 50-160\n",
    "\n",
    "present_subjects = set(ASD_phenotypic_copy['SUB_ID'])\n",
    "#Dataset original but only with actual subjects (present in ASD_phenotypic)\n",
    "filtered_original = ASD_phenotypic_original[ASD_phenotypic_original['SUB_ID'].isin(present_subjects)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start defining the condition\n",
    "condition = (filtered_original['FIQ_TEST_TYPE'] == 'DAS') | (ASD_phenotypic['FIQ'] < 50) | (ASD_phenotypic['FIQ'] > 160)\n",
    "\n",
    "# Then we standarize the values dictated by the condition, to the new scale\n",
    "ASD_phenotypic['FIQ'] = np.where(condition, \n",
    "                        (ASD_phenotypic['FIQ'] - 30) / (170 - 30) * (160 - 50) + 50, \n",
    "                        ASD_phenotypic['FIQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For VIQ, the score scale is between 31-169 if the test taken was \"DAS\", \n",
    "#between 36-164 if the test taken was \"STANFORD\",\n",
    " #between 40-160 if the test taken was \"PPVT\",  otherwise is 50-160.\n",
    "#We will unify all the data to the more common used scale, i.e. 50-160\n",
    "\n",
    "for i in ASD_phenotypic.index:\n",
    "    test_type = filtered_original['VIQ_TEST_TYPE'][i]\n",
    "    current_value = ASD_phenotypic['VIQ'][i]\n",
    "    if (test_type == 'DAS') or (current_value <36) or (current_value > 164):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 31) / (169 - 31) * (160 - 50) + 50\n",
    "    elif (test_type == 'STANFORD') or (current_value <40) or (current_value > 160):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 36) / (164 - 36) * (160 - 50) + 50\n",
    "    elif (test_type == 'PPVT') or (current_value <50) or (current_value > 160):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 40) / (160 - 40) * (160 - 50) + 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PIQ, the score scale is between 31-166 if the test taken was \"DAS\", \n",
    "#between 36-164 if the test taken was \"STANFORD\",\n",
    " #between 50-160 if the test taken was \"RAVENS\",  otherwise is 53-160.\n",
    "#We will unify all the data to the more common used scale,, i.e. 50-160\n",
    "\n",
    "for i in ASD_phenotypic.index:\n",
    "    test_type = ASD_phenotypic['PIQ_TEST_TYPE'][i]\n",
    "    current_value = ASD_phenotypic['PIQ'][i]\n",
    "    if (test_type == 'DAS') or (current_value <36) or (current_value > 164):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 31) / (166 - 31) * (160 - 50) + 50\n",
    "    elif (test_type == 'STANFORD') or (current_value <50) or (current_value > 160):\n",
    "        ASD_phenotypic.loc[i, 'VIQ'] = (current_value - 36) / (164 - 36) * (160 - 50) + 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADOS_TOTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature \"ADOS_TOTAL\" is simply the sum of the scores obtained by \"ADOS_COMM\" and \"ADOS_SOCIAL\", so we can reduce the amount of missing values using the values of those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9999 and \"-9999\" with NaN\n",
    "ASD_phenotypic_original[\"ADOS_COMM\"] = ASD_phenotypic_original[\"ADOS_COMM\"].replace(['-9999', -9999], np.NaN)\n",
    "ASD_phenotypic_original[\"ADOS_SOCIAL\"] = ASD_phenotypic_original[\"ADOS_SOCIAL\"].replace(['-9999', -9999], np.NaN)\n",
    "\n",
    "for i in ASD_phenotypic[\"ADOS_TOTAL\"].index:\n",
    "    ados_comm = ASD_phenotypic_original[\"ADOS_COMM\"][i]\n",
    "    ados_social = ASD_phenotypic_original[\"ADOS_SOCIAL\"][i]\n",
    "    if not pd.isna(ados_comm) and not pd.isna(ados_social):\n",
    "        ASD_phenotypic.loc[i, \"ADOS_TOTAL\"] = ados_comm + ados_social\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test scores filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that to fill the missing values of the test subministred\n",
    "it should be good to rely on the standard score achieved by the mean\n",
    "of the global population (if the statistics are available in the literature) or the cutoff for the diagnostic of ASD,\n",
    "otherwise we will use the mean extracted from our dataset.\n",
    "\n",
    "So for the features \"FIQ\", \"VIQ\", \"PIQ\", \"ADOS_TOTAL\", \"ADI_R_VERBAL_TOTAL_BV\", we will apply a custom function that checks if there is an available value in literature for the mundial mean, otherwise assign the mean of the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of features that we want to fill\n",
    "test_score_fatures = [\"FIQ\", \"VIQ\", \"PIQ\", \"ADOS_TOTAL\", \"ADI_R_VERBAL_TOTAL_BV\"]\n",
    "\n",
    "#function to fill with the global mean or the data feature mean\n",
    "def test_score_fill (feature_value, feature_name, feature_mean):\n",
    "    # We create a dictionary to store the literature mean scores\n",
    "    literature_scores = {\n",
    "    \"FIQ\": list(range(95, 100)), # EEUU, mean score retrieved from https://www.worlddata.info/iq-by-country.php\n",
    "    \"VIQ\": list(range(95, 100)), # EEUU, mean score retrieved from https://www.worlddata.info/iq-by-country.php\n",
    "    \"PIQ\": list(range(95, 100)), # EEUU, mean score retrieved from https://www.worlddata.info/iq-by-country.php\n",
    "    \"ADOS_TOTAL\": list(range(6, 12)), # autism cutoff retrieved from https://www.researchgate.net/figure/ADOS-maximum-score-and-cut-off-points-for-ASD-15_tbl1_361212648\n",
    "    \"ADI_R_VERBAL_TOTAL_BV\": list(range(7, 10)), # autism cutoff retrieved from https://www.researchgate.net/figure/Summary-statistics-for-ADI-R-domain-scores_tbl4_6709395\n",
    "    }\n",
    "\n",
    "    # Then we check which feature we obtained to decide if replace\n",
    "    # using the value in the dictionary ot directly the mean of the data\n",
    "    if pd.isna(feature_value):\n",
    "\n",
    "        if feature_name in literature_scores:\n",
    "            return np.random.choice(literature_scores[feature_name])\n",
    "        else:\n",
    "            return feature_mean\n",
    "    else:\n",
    "\n",
    "        return feature_value\n",
    "\n",
    "#loop for filling the features   \n",
    "for feature_name in test_score_fatures:\n",
    "    feature_mean = ASD_phenotypic_original[feature_name].mean()\n",
    "    ASD_phenotypic[feature_name] = ASD_phenotypic[feature_name].apply(test_score_fill, args=(feature_name, feature_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that if we display the information of the dataset, we have no longer presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ASD_phenotypic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check if filling Missing Values has affected the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per plottare le distribuzioni delle features\n",
    "of.plot_distributions(ASD_phenotypic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, after have worked on SITE_IT feature in order to have a better preparation for possible future works, we decide to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic = ASD_phenotypic.drop(columns=[\"SITE_ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESSED DATA STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We decide to store in a file .csv the pre-processed dataset\n",
    "ASD_phenotypic.to_csv('DataSets/Phenotypic Datasets/ASD_phenotypic_preprocessed.csv', index=False)\n",
    "# And also the diagnostic groups\n",
    "ASD_clinical.to_csv('DataSets/Phenotypic Datasets/ASD_clinical.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
