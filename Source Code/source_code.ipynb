{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# from our documents\n",
    "import OurFunctions as of\n",
    "\n",
    "# from Scikit Learn library\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, make_scorer, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import plot_tree \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# from Imb Learn\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_phenotypic = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_phenotypic_preprocessed.csv'))\n",
    "ASD_diagnosis = pd.read_csv(os.path.join('DataSets','Phenotypic Datasets','ASD_clinical.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (213, 8)\n",
      "Test set size: (92, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "ASD_phenotypic = ASD_phenotypic.drop(columns=[\"SITE_ID\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(ASD_phenotypic, ASD_diagnosis['DX_GROUP'], test_size=0.3, random_state=42)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "categorical_features = categorical_columns.tolist()\n",
    "\n",
    "# Initializing SMOTE-NC \n",
    "sampler = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "\n",
    "# Applying SMOTE-NC in order to generate new syntetic samples\n",
    "X_SMOTE, Y_SMOTE = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_pipeline(dataset, target, classifier, encoder = True, scaler = True, parameters_grid_search = None, cv = None, feature_selector=False,  parameters_feature_selector = None):\n",
    "\n",
    "    # Defining metrics to be used as scoring \n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'f1_score': make_scorer(f1_score)\n",
    "    }\n",
    "\n",
    "    # Preprocess to make on the train data may include\n",
    "    # - normalization of the numerical columns\n",
    "    # - one hot encoding on the categorical columns\n",
    "    \n",
    "    categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
    "\n",
    "    if not encoder: #only scaler\n",
    "        transformers = [('num', RobustScaler(), ~dataset.columns.isin(categorical_columns))]\n",
    "    elif not scaler: #only encoder\n",
    "        transformers = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)]\n",
    "    elif not encoder and not scaler:\n",
    "        transformers = []\n",
    "    else: # both scaler and encoder\n",
    "        transformers=[\n",
    "            ('num', RobustScaler(), ~dataset.columns.isin(categorical_columns)),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ]\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # set parameters per KNN case\n",
    "    if isinstance(classifier, KNeighborsClassifier):\n",
    "        parameter_type = 'classifier__n_neighbors'\n",
    "        if parameters_grid_search:\n",
    "            parameters = {parameter_type: parameters_grid_search}  # n_neighbors values to be explored \n",
    "        else:\n",
    "            parameters = {parameter_type: [3, 5, 7, 9, 11]} \n",
    "\n",
    "        feature_selector_type = SelectKBest(score_func=f_classif)\n",
    "        selector_parameter = 'feature_selection__k'\n",
    "        \n",
    "    # set parameters per RF case\n",
    "    if isinstance(classifier, RandomForestClassifier):\n",
    "        parameter_type = 'classifier__n_estimators'\n",
    "        if parameters_grid_search:\n",
    "            parameters = {parameter_type: parameters_grid_search}  \n",
    "        else:\n",
    "            parameters = {parameter_type: [10, 50, 200, 500, 1000]} \n",
    "\n",
    "        feature_selector_type = RFE(estimator=classifier)\n",
    "        selector_parameter = 'feature_selection__n_features_to_select'\n",
    "\n",
    "    # set parameters per LinearSVC case\n",
    "    if isinstance(classifier, CalibratedClassifierCV):\n",
    "        parameter_type = 'classifier__estimator__C'\n",
    "        if parameters_grid_search:\n",
    "            parameters = {parameter_type: parameters_grid_search}  \n",
    "        else:\n",
    "            parameters = {parameter_type: [1]}  \n",
    "\n",
    "        feature_selector_type = RFE(estimator=LinearSVC(dual=False, random_state=42))\n",
    "        selector_parameter = 'feature_selection__n_features_to_select'\n",
    "\n",
    "    \n",
    "    if feature_selector: # in case feature selection is required\n",
    "        \n",
    "        sample_transformed = preprocessor.fit_transform(dataset)\n",
    "        n_total_features = sample_transformed.shape[1]\n",
    "        print(\"Total quantity of features after one hot encoding :\" + str(n_total_features))\n",
    "        \n",
    "        # Pipeline with feature selector\n",
    "        pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', feature_selector_type),\n",
    "        ('classifier', classifier)  \n",
    "        ])\n",
    "        if parameters_feature_selector:\n",
    "            parameters.update({selector_parameter: parameters_feature_selector})\n",
    "        else: \n",
    "            parameters.update({selector_parameter: list(range(1, n_total_features+1))})\n",
    "            \n",
    "    else:\n",
    "        # Pipeline without feature selector\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', classifier),\n",
    "            ])\n",
    "\n",
    "    # allows to change k of cross validation from the outside\n",
    "    if cv:\n",
    "        k_folds = cv\n",
    "    else:\n",
    "        k_folds = 5\n",
    "\n",
    "    # Creating GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv= KFold(n_splits=k_folds, shuffle = True, random_state = 42), scoring=scoring, refit='accuracy')\n",
    "\n",
    "    # Executing \n",
    "    grid_search.fit(dataset, target)\n",
    "\n",
    "    # Extraction of results\n",
    "\n",
    "    means_accuracy = grid_search.cv_results_['mean_test_accuracy']\n",
    "    stds_accuracy = grid_search.cv_results_['std_test_accuracy']\n",
    "    means_precision = grid_search.cv_results_['mean_test_precision']\n",
    "    means_recall = grid_search.cv_results_['mean_test_recall']\n",
    "    means_f1_score = grid_search.cv_results_['mean_test_f1_score']\n",
    "    params = grid_search.cv_results_['params']\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_parameter = [grid_search.best_params_[parameter_type]]\n",
    "    best_accuracy = grid_search.best_score_\n",
    "    best_f1_score = grid_search.cv_results_['mean_test_f1_score'][grid_search.best_index_]\n",
    "    best_scores = [best_accuracy, best_f1_score]\n",
    "\n",
    "    feature_names = []\n",
    "    for feature in best_model.named_steps['preprocessor'].get_feature_names_out():\n",
    "        feature_names.append(feature)\n",
    "    # Visualization of results\n",
    "\n",
    "    if not parameters_grid_search: #the parameter is not choose from the outside\n",
    "        print(\"Cross-validation results for each combination of hyperparameters:\")\n",
    "        for mean_acc, std_acc, mean_prec, mean_rec, mean_f1, params in zip(means_accuracy, stds_accuracy, means_precision, means_recall, means_f1_score, params):\n",
    "            print(f\"Parameters: {params}, Accuracy: {mean_acc:.3f} (Â±{std_acc:.3f}), Precision: {mean_prec:.3f}, Recall: {mean_rec:.3f}, F1-score: {mean_f1:.3f}\")\n",
    "\n",
    "        print(\"\\nBest parameter founded:\")\n",
    "        print(best_parameter)\n",
    "        print(\"\\nThe scores achieved are:\")\n",
    "\n",
    "    print(\"Accuracy:\", best_accuracy)\n",
    "    print(\"F1-score:\", best_f1_score)\n",
    "\n",
    "    if feature_selector:\n",
    "        # Get the support mask and selected feature names from the preprocessed data\n",
    "        feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        selected_mask = best_model.named_steps['feature_selection'].get_support()\n",
    "        selected_features = []\n",
    "        for feature in feature_names[selected_mask]:\n",
    "            selected_features.append(feature)\n",
    "\n",
    "        print(\"\\nSelected features are:\")\n",
    "        print(f\"Number of selected features: {len(selected_features)}\")\n",
    "        print(\"Selected features:\", selected_features)\n",
    "\n",
    "        non_selected_mask = ~selected_mask\n",
    "        non_selected_features = feature_names[non_selected_mask]\n",
    "\n",
    "        print(\"\\nNon selected features are:\")\n",
    "        print(f\"Number of non selected features: {len(non_selected_features)}\")\n",
    "        print(\"Selected features:\", non_selected_features)\n",
    "\n",
    "        best_parameter.append(grid_search.best_params_[selector_parameter])\n",
    "\n",
    "        return best_model, best_parameter, best_scores, selected_features\n",
    "        \n",
    "    return best_model, best_parameter, best_scores, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Best Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest Classifier\n",
    "- 50 trees\n",
    "- No outlier remotion\n",
    "- No feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9814903846153846\n",
      "F1-score: 0.9817209205888451\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "rf_model, rf_parameter, rf_scores, _ = general_pipeline(X_SMOTE, Y_SMOTE, classifier, parameters_grid_search = [50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valutazione del modello sui dati di test:\n",
      "Accuratezza: 0.978\n",
      "Precisione: 1.000\n",
      "Richiamo: 0.970\n",
      "F1-score: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Model on Test Data \n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours: Second Best Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors\n",
    "- Best k = 3\n",
    "- No outlier remotion\n",
    "- 2 features selected in Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total quantity of features after one hot encoding :11\n",
      "Accuracy: 0.9660576923076925\n",
      "F1-score: 0.9656496087798283\n",
      "\n",
      "Selected features are:\n",
      "Number of selected features: 2\n",
      "Selected features: ['num__ADI_R_VERBAL_TOTAL_BV', 'num__ADOS_TOTAL']\n",
      "\n",
      "Non selected features are:\n",
      "Number of non selected features: 9\n",
      "Selected features: ['num__AGE_AT_SCAN' 'num__SEX' 'num__FIQ' 'num__VIQ' 'num__PIQ'\n",
      " 'cat__PIQ_TEST_TYPE_DAS' 'cat__PIQ_TEST_TYPE_WAIS'\n",
      " 'cat__PIQ_TEST_TYPE_WASI' 'cat__PIQ_TEST_TYPE_WISC']\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "knn_model, knn_parameter, knn_scores, knn_selected_features = general_pipeline(X_SMOTE, Y_SMOTE, classifier, parameters_grid_search = [3], feature_selector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valutazione del modello sui dati di test:\n",
      "Accuratezza: 0.978\n",
      "Precisione: 1.000\n",
      "Richiamo: 0.970\n",
      "F1-score: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Model on Test Data \n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Prints and stores scores of the model on testing set\n",
    "accuracy, precision, recall, f1 = of.evaluation_test_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classifier: Third Best Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Support Vector Classifier\n",
    "- Default parameter\n",
    "- No outlier remotion\n",
    "- No feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CalibratedClassifierCV(LinearSVC(dual=False, random_state=42))\n",
    "svc_model, svc_parameter, svc_scores, _ = general_pipeline(X_SMOTE, Y_SMOTE, classifier, parameters_grid_search= [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_auc_roc = of.evaluate_roc_auc(knn_model, X_test, y_test)\n",
    "rf_auc_roc = of.evaluate_roc_auc(rf_model, X_test, y_test)\n",
    "svc_auc_roc = of.evaluate_roc_auc(svc_model, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"K-Nearest Neighbors Classifier AUC-ROC:\", knn_auc_roc)\n",
    "print(\"Random Forest Classifier AUC-ROC:\", rf_auc_roc)\n",
    "print(\"Support vector Classifier AUC-ROC:\", svc_auc_roc)\n",
    "\n",
    "# We need to make the diagnostic binary (instead of 1 and 2)\n",
    "y_test_binary = (y_test == 2).astype(int)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "of.plot_roc_curve(knn_model, X_test, y_test_binary, 'K-Nearest Neighbors Classifier')\n",
    "of.plot_roc_curve(rf_model, X_test, y_test_binary, 'Random Forest Classifier')\n",
    "of.plot_roc_curve(svc_model, X_test, y_test_binary, 'Support vector Classifier')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
